<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Introduction</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="DeepLearning-Julia_files/libs/clipboard/clipboard.min.js"></script>
<script src="DeepLearning-Julia_files/libs/quarto-html/quarto.js"></script>
<script src="DeepLearning-Julia_files/libs/quarto-html/popper.min.js"></script>
<script src="DeepLearning-Julia_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="DeepLearning-Julia_files/libs/quarto-html/anchor.min.js"></script>
<link href="DeepLearning-Julia_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="DeepLearning-Julia_files/libs/quarto-html/quarto-syntax-highlighting-549806ee2085284f45b00abea8c6df48.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="DeepLearning-Julia_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="DeepLearning-Julia_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="DeepLearning-Julia_files/libs/bootstrap/bootstrap-973236bd072d72a04ee9cd82dcc9cb29.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">


</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Introduction</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Deep learning is a specialized subset of machine learning that uses multi-layered neural networks (often called deep neural networks) to learn complex patterns from data​. In this tutorial, we’ll introduce <strong>Flux.jl</strong>, a pure-Julia library for machine learning models (including deep neural networks)​, and walk through building, training, and evaluating a neural network on the classic MNIST dataset of handwritten digits.</p>
<p><strong>This tutorial covers</strong>:</p>
<ol type="1">
<li><p>Environment setup and data preprocessing</p></li>
<li><p>Building a simple neural network model</p></li>
<li><p>Training the model with mini-batching</p></li>
<li><p>Evaluating the model and plotting results</p></li>
<li><p>Advanced topics: checkpointing and mixed-precision training with an FP32 master copy</p></li>
</ol>
<p><strong>Standard ML workflow</strong>:</p>
<ul>
<li><p><strong>Data Collection &amp; Preparation</strong>: MNIST provides images of handwritten digits along with their true labels (0 through 9)</p></li>
<li><p><strong>Model Building</strong>: Neural network (a chain of layers) that will take inputs (images) and produce outputs (predicted labels)</p></li>
<li><p><strong>Training</strong>: Forward Pass (using a loss function), and adjust the model’s parameters (weights) to reduce this error (iterative gradient descent)</p></li>
<li><p><strong>Evaluation</strong>: Test set Evaluation on model’s performance (e.g., accuracy in classification)</p></li>
</ul>
<section id="what-is-flux.jl" class="level3">
<h3 class="anchored" data-anchor-id="what-is-flux.jl">What is Flux.jl</h3>
<p><a href="https://fluxml.ai/Flux.jl/stable/">Flux.jl</a> is an open-source machine learning library written solely in Julia. It provides a flexible and maths-friendly framework for building neural networks (e.g.&nbsp;feed-forward networks, convolutional networks, recurrent networks, etc.) in just a few lines of code, very similar to PyTorch. It integrates smoothly with the Julia language and has noticeable features such as automatic differentiation (AD) and CUDA support.</p>
<p>Some key characteristics of Flux.jl:</p>
<ul>
<li><p><strong>Easy Model Definition</strong>: You can define models like writing simple Julia functions. Flux provides layers like <code>Dense</code> (fully connected layer), <code>Conv</code> (convolutional layer), activation functions (like <code>relu</code>, <code>sigmoid</code>), and utilities to chain them together. (more example to come)</p></li>
<li><p><strong>Gradients and Training</strong>: Flux handles backpropagation (gradient calculation) for you via Julia’s automatic differentiation (AD) system. You just define a loss function, and Flux can compute gradients and update parameters using optimizers (like SGD, Adam).</p></li>
<li><p><strong>Integration with Julia Ecosystem</strong>: Rather than being a monolithic framework, Flux works with other Julia packages. For example, it uses MLDatasets.jl for easy data loading (which we’ll use for MNIST) and allows using any Julia array type (CPU or GPU arrays, etc.) seamlessly​</p></li>
</ul>
</section>
<section id="example" class="level2">
<h2 class="anchored" data-anchor-id="example">Example</h2>
<section id="environment-set-up" class="level3">
<h3 class="anchored" data-anchor-id="environment-set-up">Environment Set Up</h3>
<p>We’ll use <strong>Flux</strong> for deep learning, <strong>CUDA</strong> for GPU support, and <strong>Plots</strong> for visualization. We’ll also use the <strong>MLDatasets</strong> which contains common datasets including MNIST</p>
<div id="cell-8" class="cell">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># using Pkg</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Pkg.add("Flux")       </span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Pkg.add("MLDatasets") </span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-9" class="cell">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Flux</span>            </span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">MLDatasets</span>      </span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co"># MNIST dataset</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a>train_x, train_y <span class="op">=</span> MLDatasets.MNIST.<span class="fu">traindata</span>();</span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>test_x,  test_y  <span class="op">=</span> MLDatasets.MNIST.<span class="fu">testdata</span>();   </span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="fu">println</span>(<span class="fu">size</span>(train_x))   <span class="co"># Expect (28, 28, 60000) - 60k images of 28x28 pixels</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="fu">println</span>(<span class="fu">size</span>(train_y))   <span class="co"># Expect (60000,) - 60k labels corresponding to the images</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="fu">println</span>(<span class="fu">size</span>(test_x))    <span class="co"># Expect (28, 28, 10000) - 10k test images</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="fu">println</span>(<span class="fu">size</span>(test_y))    <span class="co"># Expect (10000,) - 10k test labels</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><img src="Minist.png" alt="Loss Curve" style="max-width:100%; width:600px; height:auto;"></p>
<p><strong>Explanation</strong>: The MNIST dataset consists of grayscale images of handwritten digits 0 through 9, each image being 28x28 pixels​. The code above loads <code>train_x</code> and <code>train_y</code> as the training set images and labels, and similarly <code>test_x</code> and <code>test_y</code> for the test set. According to the dataset, there are 60,000 training examples and 10,000 test examples​, which matches the printed shapes:</p>
<ul>
<li><p><code>train_x</code> is a 28×28×60000 array (the third dimension indexes the images).</p></li>
<li><p><code>train_y</code> is a vector of length 60000, containing the digit labels (0–9) for each training image.</p></li>
<li><p>The test set shapes are 28×28×10000 for images and 10000 for labels.</p></li>
</ul>
<p><strong>Preprocessing step</strong>: in this case, <code>train_x</code> and <code>test_x</code> might be of type <code>N0f8</code> (normalized 8-bit fixed-point) or <code>UInt8</code> representing pixel intensities. We’ll convert them to <code>Float32</code> for Flux, and also normalize pixel values to the 0-1 range if they aren’t already. (Often, <code>MLDatasets</code> provides the images already as Float32 in [0,1].)</p>
<p><strong>One-Hot Encoding</strong>: Tune labels for multi-class classification​. It means representing each label as a vector of length 10 (for digits 0-9) that has a 1 in the position corresponding to the digit and 0 in all other positions. For example, label <code>3</code> becomes <code>[0,0,0,1,0,0,0,0,0,0]</code>. Flux provides a convenient <code>onehotbatch</code> function to do this conversion for a batch of labels.</p>
<div id="cell-14" class="cell">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>train_x <span class="op">=</span> <span class="fu">Float32</span>.(train_x);  </span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>test_x  <span class="op">=</span> <span class="fu">Float32</span>.(test_x);</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co"># One-hot encode</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>train_y_onehot <span class="op">=</span> Flux.<span class="fu">onehotbatch</span>(train_y, <span class="fl">0</span><span class="op">:</span><span class="fl">9</span>);  <span class="co"># 10×60000 matrix of one-hot columns</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>test_y_onehot  <span class="op">=</span> Flux.<span class="fu">onehotbatch</span>(test_y, <span class="fl">0</span><span class="op">:</span><span class="fl">9</span>);   <span class="co"># 10×10000 one-hot encoded labels for test</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="fu">println</span>(train_y[<span class="fl">1</span>], <span class="st">" -&gt; "</span>, <span class="fu">vec</span>(train_y_onehot[<span class="op">:</span>,<span class="fl">1</span>])) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><img src="OneHot.png" alt="My Plot" style="max-width:100%; width:800px; height:auto;"></p>
<p><strong>Explanation</strong>: We used <code>Float32.(train_x)</code> to broadcast conversion of each element to <code>Float32</code>.</p>
<p>If the original pixel values were 0-255, converting to <code>Float32</code> will yield 0.0-255.0; if they were <code>N0f8</code> (0.0-1.0 in an 8-bit format), the conversion yields 0.0-1.0 floats. In either case, a neural network can work with these scaled inputs (though if it were 0-255, we might explicitly divide by 255 to scale to [0,1]). This may be an issue when coming into lower precision such as <code>Float16</code> as we will discuss later.</p>
<p>One-hot encoding is done via <code>Flux.onehotbatch(labels, 0:9)</code>, which produces a 10-row matrix where each column is a one-hot representation of the corresponding label</p>
<p><strong>Move to GPU</strong></p>
<div id="cell-18" class="cell">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">CUDA</span>  </span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> CUDA.<span class="fu">has_cuda</span>()</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    <span class="fu">println</span>(<span class="st">"CUDA is available! Using GPU for computations."</span>)</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    <span class="fu">device</span>(x) <span class="op">=</span> <span class="fu">cu</span>(x)  <span class="co"># GPU</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="cf">else</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">println</span>(<span class="st">"CUDA not available. Using CPU."</span>)</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="fu">device</span>(x) <span class="op">=</span> x      <span class="co"># CPU</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a><span class="cf">end</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-19" class="cell">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>train_x_gpu <span class="op">=</span> <span class="fu">device</span>(train_x);</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>train_y_gpu <span class="op">=</span> <span class="fu">device</span>(train_y);</span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>test_x_gpu <span class="op">=</span> <span class="fu">device</span>(test_x);</span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>test_y_gpu <span class="op">=</span> <span class="fu">device</span>(test_y);</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>train_y_onehot_gpu <span class="op">=</span> <span class="fu">device</span>(train_y_onehot);</span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>test_y_onehot_gpu <span class="op">=</span> <span class="fu">device</span>(test_y_onehot);</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As a side note:</p>
<p><code>cu(x)</code> is a generic, higher-level conversion function that “adapts” its input to a GPU array. If <code>x</code> is already a <code>CuArray</code>, it simply returns it unchanged. And if <code>x</code> is a CPU array, it’s equivalent to calling <code>CuArray(x)</code></p>
<p><code>CuArray(x)</code> explicit construct GPU arrays which make it transparent for speed / memory demostration.</p>
</section>
<section id="building-a-neural-network-model-with-flux" class="level3">
<h3 class="anchored" data-anchor-id="building-a-neural-network-model-with-flux">Building a Neural Network Model with Flux</h3>
<p>Flux makes it easy to define neural network models —similar in spirit to the <code>custom nn.Module</code> example in PyTorch - using the <code>Chain</code> function, which combines layers (and functions) sequentially. A neural network can be thought of as a chain of layers, where each layer transforms its input to some output; these outputs become inputs to the next layer​. In Flux, <code>Chain</code> takes a list of layer constructors (or functions) and creates a callable model.</p>
<p>For our MNIST classifier, we’ll build a simple feed-forward neural network (multi-layer perceptron) with one hidden layer:</p>
<ul>
<li><p><strong>Input layer</strong>: 28×28 pixels per image, which we will <strong>flatten</strong> into a 784-dimensional vector (this can be done with <code>Flux.flatten</code> as a layer in the chain)</p></li>
<li><p><strong>Hidden layer</strong>: a fully connected dense layer with, say, 128 neurons and a ReLU activation. This layer will take the 784-dim input and produce 128 outputs.</p></li>
<li><p><strong>Output layer</strong>: a dense layer with 10 neurons (one for each digit class) producing the raw scores for each class. We’ll later apply a softmax or appropriate loss to these scores to get class probabilities.</p></li>
</ul>
<div id="cell-23" class="cell">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Flux</span>: Dense, Chain, relu, flatten  </span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> <span class="fu">Chain</span>(</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>    flatten,                 <span class="co"># flatten 28x28 input images into 784-element vectors</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>    <span class="fu">Dense</span>(<span class="fl">28</span><span class="op">*</span><span class="fl">28</span>, <span class="fl">128</span>, relu), <span class="co"># hidden layer: 784 -&gt; 128, with ReLU activation</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    <span class="fu">Dense</span>(<span class="fl">128</span>, <span class="fl">10</span>)           <span class="co"># output layer: 128 -&gt; 10 (raw scores for 10 digits)</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>model_gpu <span class="op">=</span> <span class="fu">device</span>(model)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>We used <code>Chain</code> to stack three components: <code>flatten</code> (to convert 2D image to 1D vector), a <code>Dense</code> layer with 128 neurons and ReLU, and another <code>Dense</code> layer with 10 outputs. The <code>Dense(input_dim, output_dim, activation)</code> constructor creates a fully connected layer (it automatically initializes weights and biases). Here, the first Dense layer takes 784 inputs and gives 128 outputs with ReLU activation applied, and the second Dense layer takes 128 inputs and gives 10 outputs. We did not specify an activation for the output layer; in classification tasks, it’s common to apply a softmax at the end to interpret outputs as probabilities​, but we’ll handle that in the loss function for numerical stability instead of as a separate layer. (Alternatively, one could add <code>softmax</code> as the final layer in the Chain, but then one should use a corresponding loss that expects probabilities.)</p>
<p>Also, note that each layer like <code>Dense</code> is an ordinary struct, which encapsulates some arrays of parameters (and possibly other state, as for <code>BatchNorm</code>)</p>
<pre><code>julia&gt; typeof(Dense(28*28, 128, relu))

Dense{typeof(relu), Matrix{Float32}, Vector{Float32}}</code></pre>
<p>indicates that the layer uses the relu activation function, with a weight matrix of type Matrix{Float32} and a bias vector of type Vector{Float32}</p>
<p>As discussed in <a href="https://stat244.berkeley.edu/spring-2025/notes/notes8.html">Notes 8 GPUs</a>, we could write custom CUDA kernels in Julia to accelerate operations in training loop. Given that most operations (matrix multiples, convolutions, etc.) are highly optimized using libraries like cuDNN already, one common target is the <strong>activation function</strong>. A simple demo in Appendix.</p>
<p>At this point, <code>model</code> is a Flux model that we can call like a function. For example, <code>model(train_x[:,:,1:5])</code> would output predictions for the first 5 images (though they would be untrained, random predictions initially). We haven’t trained the model yet, so if we tried to predict, the outputs would be basically random relative to the true labels​. Training will adjust the weights to make these predictions meaningful.</p>
<section id="gpu-cpu-tensor" class="level4">
<h4 class="anchored" data-anchor-id="gpu-cpu-tensor">GPU &amp; CPU Tensor</h4>
<div id="cell-28" class="cell">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="pp">@time</span> cpu_predictions <span class="op">=</span> <span class="fu">model</span>(test_x);</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 2.668480 seconds (2.98 M allocations: 211.474 MiB, 79.97% compilation time)</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 0.610637 seconds (10 allocations: 10.529 MiB)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-29" class="cell">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="pp">@time</span> gpu_predictions <span class="op">=</span> <span class="fu">model_gpu</span>(test_x_gpu);</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="co"># 3.151836 seconds (2.50 M allocations: 166.681 MiB, 2.00% gc time, 89.35% compilation time)</span></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co"># 0.000341 seconds (298 allocations: 6.703 KiB)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="performance-boost-with-lower-precision" class="level4">
<h4 class="anchored" data-anchor-id="performance-boost-with-lower-precision">Performance Boost with Lower Precision</h4>
<div id="cell-31" class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Matrix multiplication with Float64</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>A64 <span class="op">=</span> <span class="fu">rand</span>(<span class="fl">10000</span>, <span class="fl">10000</span>);</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>B64 <span class="op">=</span> <span class="fu">rand</span>(<span class="fl">10000</span>, <span class="fl">10000</span>);</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a><span class="pp">@time</span> C64 <span class="op">=</span> A64 <span class="op">*</span> B64;</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 25.654823 seconds (2 allocations: 762.939 MiB, 0.27% gc time)</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Matrix multiplication with Float32</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>A32 <span class="op">=</span> <span class="fu">rand</span>(<span class="dt">Float32</span>, <span class="fl">10000</span>, <span class="fl">10000</span>);</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>B32 <span class="op">=</span> <span class="fu">rand</span>(<span class="dt">Float32</span>, <span class="fl">10000</span>, <span class="fl">10000</span>);</span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a><span class="pp">@time</span> C32 <span class="op">=</span> A32 <span class="op">*</span> B32;</span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 13.297140 seconds (2 allocations: 381.470 MiB)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="gradient" class="level3">
<h3 class="anchored" data-anchor-id="gradient">Gradient</h3>
<p>Flux leverages <a href="https://fluxml.ai/Zygote.jl/dev/">Zygote</a> for source-to-source automatic differentiation (AD), which means that the <code>gradient(f, x)</code> connects to Julia’s compiler to transform operations in <code>f</code> to produce code for computing <code>∂f/∂x</code>. Whether you work with implicit global parameters or pass them explicitly, <a href="https://fluxml.ai/Flux.jl/stable/guide/models/basics/">Flux</a> uses AD to compute gradients efficiently.</p>
<section id="example-1-implicit-parameters-with-global-variables" class="level4">
<h4 class="anchored" data-anchor-id="example-1-implicit-parameters-with-global-variables">Example 1: Implicit Parameters with Global Variables</h4>
<p>Defines a polynomial using a closure over the global vector <code>θ</code>. When we call <code>gradient(poly, 5.0)</code>, Flux computes the derivative of <code>poly</code> at 5.</p>
<div id="cell-35" class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Flux</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>θ <span class="op">=</span> [<span class="fl">10.0</span>, <span class="fl">1.0</span>, <span class="fl">0.1</span>]; <span class="co"># Global parameters for a quadratic polynomial</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="fu">poly</span>(x) <span class="op">=</span> θ[<span class="fl">1</span>] <span class="op">+</span> θ[<span class="fl">2</span>]<span class="op">*</span>x <span class="op">+</span> θ[<span class="fl">3</span>]<span class="op">*</span>x<span class="op">^</span><span class="fl">2</span>;</span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradient of poly with respect to input x at x = 5</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>grad_input <span class="op">=</span> <span class="fu">gradient</span>(poly, <span class="fl">5.0</span>); <span class="co"># d(poly)/dx.</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="fu">println</span>(<span class="st">"Gradient with respect to input: "</span>, grad_input[<span class="fl">1</span>])</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a><span class="co"># d/dx (10 + x + 0.1*x^2) = 1 + 0.2*x, and at x=5, 1+1=2)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Gradient with respect to input: 2.0</code></pre>
</div>
</div>
</section>
<section id="example-2-explicit-parameter-passing" class="level4">
<h4 class="anchored" data-anchor-id="example-2-explicit-parameter-passing">Example 2: Explicit Parameter Passing</h4>
<p>Defines <code>poly2</code> where parameters are explicitly passed. This allows Flux to return a tuple with gradients with respect to both the input and the parameters.</p>
<div id="cell-37" class="cell">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Alternative version that takes parameters explicitly.</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="fu">poly2</span>(x, θ<span class="fl">2</span>) <span class="op">=</span> <span class="fu">evalpoly</span>(x, θ<span class="fl">2</span>); <span class="co"># built-in, from Base.Math</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Gradients with respect to both input and parameters.</span></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>grad_input_param <span class="op">=</span> <span class="fu">gradient</span>(poly2, <span class="fl">5.0</span>, θ);</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a><span class="fu">println</span>(<span class="st">"Gradient with respect to x: "</span>, grad_input_param[<span class="fl">1</span>])</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a><span class="fu">println</span>(<span class="st">"Gradient with respect to parameters: "</span>, grad_input_param[<span class="fl">2</span>])</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Grad_input_param[2] is a vector of derivatives [∂poly2/∂θ[1], ∂poly2/∂θ[2], ∂poly2/∂θ[3]]</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Gradient with respect to input: 2.0
Gradient with respect to x: 2.0
Gradient with respect to parameters: [1.0, 5.0, 25.0]</code></pre>
</div>
</div>
<p><code>Gradient</code> is also used within <code>train!</code> to differentiate the model.</p>
</section>
</section>
<section id="defining-the-loss-function-and-optimizer" class="level3">
<h3 class="anchored" data-anchor-id="defining-the-loss-function-and-optimizer">Defining the Loss Function and Optimizer</h3>
<p><strong>Loss Function</strong>: For a multi-class classification problem like MNIST, a common choice of loss function is <em>cross-entropy loss</em>. Cross-entropy measures the difference between two probability distributions: in our case, the model’s predicted distribution over classes vs.&nbsp;the true distribution (which for a correct label is a one-hot vector)​. We will use Flux’s built-in cross-entropy implementations. Specifically, we will use <code>Flux.Losses.logitcrossentropy</code>, which expects the model’s <strong>raw output scores (logits)</strong> and the true one-hot vector, and internally applies the softmax and cross-entropy in a numerically stable way. This is equivalent to applying a softmax to get probabilities and then using crossentropy, but <code>logitcrossentropy</code> is preferred to avoid potential numerical issues.</p>
<p><strong>Optimizer</strong>: We need to choose an optimization algorithm to update the model’s weights based on the gradients of the loss. A good default for neural networks is <strong>Adam</strong> (Adaptive Moment Estimation), which often converges faster than basic stochastic gradient descent​. We’ll use Flux’s <strong>ADAM optimizer</strong> with a modest learning rate.</p>
<p>We also need to gather the model parameters into a container that the optimizer will update. Flux provides <code>params(model)</code> for that.</p>
<div id="cell-42" class="cell">
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Flux</span>: onecold  </span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="fu">loss_function</span>(x, y) <span class="op">=</span> Flux.Losses.<span class="fu">logitcrossentropy</span>(<span class="fu">model</span>(x), y);  </span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> Flux.Optimise.<span class="fu">ADAM</span>(<span class="fl">0.001</span>);  <span class="co"># Adam optimizer with learning rate 0.001</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>parameters <span class="op">=</span> Flux.<span class="fu">params</span>(model); <span class="co"># Trainable parameters</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>A quick breakdown:</p>
<ul>
<li><p><code>loss_function(x, y)</code> runs our <code>model</code> on input <code>x</code> and compares the output to true one-hot label <code>y</code> using logit cross-entropy. This will give a scalar loss value (or average loss over a batch).</p></li>
<li><p><code>opt = ADAM(0.001)</code> creates an Adam optimizer. (Accessed via <code>Flux.Optimise.ADAM</code> – note some Flux versions you might use <code>ADAM()</code> if properly imported. The exact namespace isn’t too important, as long as we have an <code>opt</code> object.)</p></li>
<li><p><code>parameters = Flux.params(model)</code> collects all the weight and bias arrays from our model layers. Flux will use this to know what values to update during training.</p></li>
</ul>
<p>Now we have all components ready for training: the model (<code>model</code>), the objective (<code>loss_function</code>), the data (<code>train_x</code> and <code>train_y_onehot</code>), and the optimizer (<code>opt</code>)​. We can proceed to train the model.</p>
</section>
<section id="training-the-model" class="level3">
<h3 class="anchored" data-anchor-id="training-the-model">Training the Model</h3>
<p>Training a model in Flux typically involves iterating over the dataset multiple times (epochs) and updating the model parameters to gradually reduce the loss​. We have a few options for how to implement the training loop:</p>
<ul>
<li><p>Use <code>Flux.train!</code>, a convenience function that automates the loop over data points or batches</p></li>
<li><p>Manually write a loop using <code>Flux.gradient</code> and update the parameters (As we will see in Model Checkpointing example).</p></li>
<li><p>Use mini-batches for efficiency (especially for large datasets) via <code>Flux.Data.DataLoader</code> to batch and shuffle data​</p></li>
</ul>
<p>For simplicity, we’ll use <code>train!</code> along with a DataLoader for mini-batching. This way, we can train in batches (say 128 images at a time) rather than one image at a time or the entire dataset at once, compromising speed and stability.</p>
<p>Let’s create a DataLoader for our training data and run a training loop for a certain number of epochs:</p>
<div id="cell-46" class="cell">
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Flux.Data</span>: DataLoader</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Plots</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Data loader for mini-batch iteration</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="fl">128</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> <span class="fu">DataLoader</span>((train_x, train_y_onehot), batchsize<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="cn">true</span>)</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>train_losses <span class="op">=</span> <span class="dt">Float32</span>[]</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="fl">5</span> </span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span>epochs</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (x_batch, y_batch) <span class="kw">in</span> train_loader</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>        Flux.<span class="fu">train!</span>(loss_function, parameters, [(x_batch, y_batch)], opt)</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">end</span></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>    train_loss <span class="op">=</span> <span class="fu">loss_function</span>(train_x[<span class="op">:</span>, <span class="op">:</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">1000</span>], train_y_onehot[<span class="op">:</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">1000</span>])  <span class="co"># loss on a subset</span></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>    <span class="fu">push!</span>(train_losses, train_loss)</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>    <span class="fu">println</span>(<span class="st">"Epoch </span><span class="sc">$</span>epoch<span class="st"> complete. Sample training loss = </span><span class="sc">$</span>(train_loss)<span class="st">."</span>)</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a><span class="cf">end</span></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a>p1 <span class="op">=</span> <span class="fu">plot</span>(train_losses, </span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>          title<span class="op">=</span><span class="st">"Training Loss"</span>, </span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>          label<span class="op">=</span><span class="st">"Loss"</span>, </span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>          xlabel<span class="op">=</span><span class="st">"Epoch"</span>, </span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>          ylabel<span class="op">=</span><span class="st">"Loss"</span>, </span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>          linewidth<span class="op">=</span><span class="fl">2</span>, </span>
<span id="cb16-26"><a href="#cb16-26" aria-hidden="true" tabindex="-1"></a>          marker<span class="op">=:</span>circle)</span>
<span id="cb16-27"><a href="#cb16-27" aria-hidden="true" tabindex="-1"></a><span class="fu">savefig</span>(p1, <span class="st">"loss_plot.png"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><img src="loss_plot.png" alt="Loss Curve" style="max-width:100%; width:600px; height:auto;"></p>
<p><strong>Explanation</strong>: We constructed <code>train_loader</code> by passing a tuple <code>(train_x, train_y_onehot)</code> to <code>DataLoader</code>, with a batch size of 128 and <code>shuffle=true</code> to randomize order each epoch. In the training loop, for each epoch we loop over <code>train_loader</code>, which yields batches <code>(x_batch, y_batch)</code> of size 128. We then call <code>Flux.train!(loss_function, parameters, [(x_batch, y_batch)], opt)</code> to perform parameter updates for that batch. The <code>train!</code> function will compute the gradient of <code>loss_function(x_batch, y_batch)</code> with respect to <code>parameters</code> and update them using the Adam optimizer​.</p>
<p>We print sample training loss on a subset of data after each epoch. As the epochs progress, we observe the loss decreasing, indicating that the model is learning.</p>
<p><strong>Note</strong>: With 5 epochs and a batch size of 128 on 60k examples, that’s about 5 * (60000/128) ≈ 5 * 469 batches, which is quite manageable on CPU for a small network and even faster with GPU (CuArrays).</p>
<p><strong>Note</strong>: Training process above is its <strong>in-place</strong> modification of the model parameters, as demonstrated by the successive execution of two training phases: an initial run with 5 epochs followed by a subsequent run with 10 epochs. Flux performs parameter updates directly on the existing parameters object, accessed via <code>Flux.params(model)</code>.</p>
</section>
<section id="evaluating-the-model" class="level3">
<h3 class="anchored" data-anchor-id="evaluating-the-model">Evaluating the Model</h3>
<p>After training, we should assess how well the model generalizes to unseen data — in this case, the 10,000 images in our test set (which were not used for training). We’ll use the trained model to predict labels for the test images and then compute the accuracy: the fraction of images for which the predicted label matches the true label. Few ways to get predictions:</p>
<ul>
<li><p>We could get the raw scores from the model and take the index of the highest score (since the highest logit corresponds to the most likely class).</p></li>
<li><p>Since we one-hot encoded the test labels, we can also compare one-hot predictions to the true one-hot vectors.</p></li>
</ul>
<p>Flux provides a utility <code>onecold</code> which is the inverse of one-hot encoding — it can take the model’s probability output (or logits) and return the predicted class labels​</p>
<div id="cell-51" class="cell">
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>y_pred_logits <span class="op">=</span> <span class="fu">model</span>(test_x);                       <span class="co"># outputs (logits) for each test image</span></span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>predicted_labels <span class="op">=</span> <span class="fu">onecold</span>(y_pred_logits, <span class="fl">0</span><span class="op">:</span><span class="fl">9</span>);      <span class="co"># digit (0-9)</span></span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>true_labels <span class="op">=</span> test_y;</span>
<span id="cb17-5"><a href="#cb17-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb17-6"><a href="#cb17-6" aria-hidden="true" tabindex="-1"></a>accuracy <span class="op">=</span> <span class="fu">sum</span>(predicted_labels <span class="op">.==</span> true_labels) <span class="op">/</span> <span class="fu">length</span>(true_labels);</span>
<span id="cb17-7"><a href="#cb17-7" aria-hidden="true" tabindex="-1"></a><span class="fu">println</span>(<span class="st">"Test Accuracy: "</span>, accuracy)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Here, <code>model(test_x)</code> produces a 10×10000 matrix of raw scores (each column corresponds to one test image). The <code>onecold(..., 0:9)</code> function finds the index of the largest score in each column and maps it to the corresponding label in <code>0:9</code>​. This gives a vector of 10000 predicted digit labels. We then compare this to the true labels and compute the proportion that are equal. The result is the accuracy (a number between 0 and 1, where 1.0 would mean 100% correct).</p>
</section>
</section>
<section id="model-checkpointing-saving-and-loading" class="level2">
<h2 class="anchored" data-anchor-id="model-checkpointing-saving-and-loading">Model Checkpointing (Saving and Loading)</h2>
<p>Long training runs should save model checkpoints periodically (e.g., every few epochs) to guard against crashes and to enable later analysis or fine-tuning. Rather than saving the entire model (which can lead to compatibility issues over time), a recommended approach is to save only the model’s parameters. In our example, we use the <a href="https://github.com/JuliaIO/BSON.jl">BSON</a> format to store a CPU copy of the model’s parameters along with the current epoch. This is achieved by converting the model parameters to CPU-friendly Float32 arrays using <code>cpu.(Flux.params(model))</code>.</p>
<p>Later,to resume training or fine-tune the model, we reload the checkpoint using BSON, move the parameters back to the GPU (if available) with <code>cu.(model_params)</code>, and then update the model with <code>Flux.loadparams!(model, model_params)</code>.</p>
<div id="cell-56" class="cell">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">MLDatasets</span></span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">CUDA</span></span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Flux</span></span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Flux.Data</span>: DataLoader</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Flux.Losses</span>: logitcrossentropy</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Flux.Optimise</span>: ADAM</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">BSON</span></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Plots</span></span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Flux</span>: Dense, Chain, relu, flatten, gpu, fmap</span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a><span class="co"># Utility: move to GPU if available</span></span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a><span class="fu">device</span>(x) <span class="op">=</span> CUDA.<span class="fu">has_cuda</span>() ? <span class="fu">cu</span>(x) <span class="op">:</span> x</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Data loading and preprocessing (used in both training and reloading)</span></span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">load_data</span>()</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Load MNIST data</span></span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>    train_x, train_y <span class="op">=</span> MLDatasets.MNIST.<span class="fu">traindata</span>()</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a>    test_x,  test_y  <span class="op">=</span> MLDatasets.MNIST.<span class="fu">testdata</span>()</span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert images to Float32 and send to device (GPU if available)</span></span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>    train_x <span class="op">=</span> <span class="fu">device</span>(<span class="fu">Float32</span>.(train_x))</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>    test_x  <span class="op">=</span> <span class="fu">device</span>(<span class="fu">Float32</span>.(test_x))</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-24"><a href="#cb18-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># One-hot encode labels and send to device</span></span>
<span id="cb18-25"><a href="#cb18-25" aria-hidden="true" tabindex="-1"></a>    train_y_onehot <span class="op">=</span> <span class="fu">device</span>(Flux.<span class="fu">onehotbatch</span>(train_y, <span class="fl">0</span><span class="op">:</span><span class="fl">9</span>))</span>
<span id="cb18-26"><a href="#cb18-26" aria-hidden="true" tabindex="-1"></a>    test_y_onehot  <span class="op">=</span> <span class="fu">device</span>(Flux.<span class="fu">onehotbatch</span>(test_y, <span class="fl">0</span><span class="op">:</span><span class="fl">9</span>))</span>
<span id="cb18-27"><a href="#cb18-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-28"><a href="#cb18-28" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_x, train_y_onehot, test_x, test_y_onehot</span>
<span id="cb18-29"><a href="#cb18-29" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb18-30"><a href="#cb18-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-31"><a href="#cb18-31" aria-hidden="true" tabindex="-1"></a><span class="co"># Build the model and send it to the appropriate device</span></span>
<span id="cb18-32"><a href="#cb18-32" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">build_model</span>()</span>
<span id="cb18-33"><a href="#cb18-33" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> <span class="fu">Chain</span>(</span>
<span id="cb18-34"><a href="#cb18-34" aria-hidden="true" tabindex="-1"></a>        flatten,</span>
<span id="cb18-35"><a href="#cb18-35" aria-hidden="true" tabindex="-1"></a>        <span class="fu">Dense</span>(<span class="fl">28</span><span class="op">*</span><span class="fl">28</span>, <span class="fl">128</span>, relu),</span>
<span id="cb18-36"><a href="#cb18-36" aria-hidden="true" tabindex="-1"></a>        <span class="fu">Dense</span>(<span class="fl">128</span>, <span class="fl">10</span>)</span>
<span id="cb18-37"><a href="#cb18-37" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb18-38"><a href="#cb18-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fu">device</span>(model)</span>
<span id="cb18-39"><a href="#cb18-39" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb18-40"><a href="#cb18-40" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-41"><a href="#cb18-41" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the loss function</span></span>
<span id="cb18-42"><a href="#cb18-42" aria-hidden="true" tabindex="-1"></a><span class="fu">loss_function</span>(model, x, y) <span class="op">=</span> <span class="fu">logitcrossentropy</span>(<span class="fu">model</span>(x), y)</span>
<span id="cb18-43"><a href="#cb18-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-44"><a href="#cb18-44" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop with optional checkpointing</span></span>
<span id="cb18-45"><a href="#cb18-45" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">train_model!</span>(model, train_x, train_y; </span>
<span id="cb18-46"><a href="#cb18-46" aria-hidden="true" tabindex="-1"></a>                      epochs<span class="op">=</span><span class="fl">5</span>, batch_size<span class="op">=</span><span class="fl">128</span>, </span>
<span id="cb18-47"><a href="#cb18-47" aria-hidden="true" tabindex="-1"></a>                      checkpoint_interval<span class="op">=</span><span class="fl">5</span>, checkpoint_prefix<span class="op">=</span><span class="st">"checkpoint_epoch"</span>)</span>
<span id="cb18-48"><a href="#cb18-48" aria-hidden="true" tabindex="-1"></a>    opt <span class="op">=</span> <span class="fu">ADAM</span>(<span class="fl">0.0001</span>)</span>
<span id="cb18-49"><a href="#cb18-49" aria-hidden="true" tabindex="-1"></a>    params <span class="op">=</span> Flux.<span class="fu">params</span>(model)</span>
<span id="cb18-50"><a href="#cb18-50" aria-hidden="true" tabindex="-1"></a>    train_loader <span class="op">=</span> <span class="fu">DataLoader</span>((train_x, train_y), batchsize<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="cn">true</span>)</span>
<span id="cb18-51"><a href="#cb18-51" aria-hidden="true" tabindex="-1"></a>    current_epoch <span class="op">=</span> <span class="fl">0</span></span>
<span id="cb18-52"><a href="#cb18-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-53"><a href="#cb18-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span>epochs</span>
<span id="cb18-54"><a href="#cb18-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (x_batch, y_batch) <span class="kw">in</span> train_loader</span>
<span id="cb18-55"><a href="#cb18-55" aria-hidden="true" tabindex="-1"></a>            grads <span class="op">=</span> <span class="fu">gradient</span>(() <span class="op">-&gt;</span> <span class="fu">loss_function</span>(model, x_batch, y_batch), params)</span>
<span id="cb18-56"><a href="#cb18-56" aria-hidden="true" tabindex="-1"></a>            Flux.Optimise.<span class="fu">update!</span>(opt, params, grads)</span>
<span id="cb18-57"><a href="#cb18-57" aria-hidden="true" tabindex="-1"></a>        <span class="cf">end</span></span>
<span id="cb18-58"><a href="#cb18-58" aria-hidden="true" tabindex="-1"></a>        current_epoch <span class="op">+=</span> <span class="fl">1</span></span>
<span id="cb18-59"><a href="#cb18-59" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-60"><a href="#cb18-60" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute a sample loss on a fixed subset</span></span>
<span id="cb18-61"><a href="#cb18-61" aria-hidden="true" tabindex="-1"></a>        subset_x <span class="op">=</span> train_x[<span class="op">:</span>, <span class="op">:</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">1000</span>]</span>
<span id="cb18-62"><a href="#cb18-62" aria-hidden="true" tabindex="-1"></a>        subset_y <span class="op">=</span> train_y[<span class="op">:</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">1000</span>]</span>
<span id="cb18-63"><a href="#cb18-63" aria-hidden="true" tabindex="-1"></a>        current_loss <span class="op">=</span> <span class="fu">loss_function</span>(model, subset_x, subset_y)</span>
<span id="cb18-64"><a href="#cb18-64" aria-hidden="true" tabindex="-1"></a>        <span class="fu">println</span>(<span class="st">"Epoch </span><span class="sc">$</span>(current_epoch)<span class="st"> complete. Sample training loss = </span><span class="sc">$</span>(current_loss)<span class="st">"</span>)</span>
<span id="cb18-65"><a href="#cb18-65" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-66"><a href="#cb18-66" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Save checkpoint at the given interval or on final epoch</span></span>
<span id="cb18-67"><a href="#cb18-67" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (current_epoch <span class="op">%</span> checkpoint_interval <span class="op">==</span> <span class="fl">0</span>) <span class="op">||</span> (epoch <span class="op">==</span> epochs)</span>
<span id="cb18-68"><a href="#cb18-68" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Save CPU version of model parameters (optimizer state is not saved)</span></span>
<span id="cb18-69"><a href="#cb18-69" aria-hidden="true" tabindex="-1"></a>            model_params <span class="op">=</span> <span class="fu">cpu</span>.(Flux.<span class="fu">params</span>(model))</span>
<span id="cb18-70"><a href="#cb18-70" aria-hidden="true" tabindex="-1"></a>            filename <span class="op">=</span> <span class="st">"</span><span class="sc">$</span>(checkpoint_prefix)<span class="sc">$</span>(current_epoch)<span class="st">.bson"</span></span>
<span id="cb18-71"><a href="#cb18-71" aria-hidden="true" tabindex="-1"></a>            BSON.<span class="pp">@save</span> filename model_params current_epoch</span>
<span id="cb18-72"><a href="#cb18-72" aria-hidden="true" tabindex="-1"></a>            <span class="fu">println</span>(<span class="st">"Checkpoint saved for epoch </span><span class="sc">$</span>(current_epoch)<span class="st"> in file </span><span class="sc">$</span>(filename)<span class="st">"</span>)</span>
<span id="cb18-73"><a href="#cb18-73" aria-hidden="true" tabindex="-1"></a>        <span class="cf">end</span></span>
<span id="cb18-74"><a href="#cb18-74" aria-hidden="true" tabindex="-1"></a>    <span class="cf">end</span></span>
<span id="cb18-75"><a href="#cb18-75" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-76"><a href="#cb18-76" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model, current_epoch</span>
<span id="cb18-77"><a href="#cb18-77" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb18-78"><a href="#cb18-78" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-79"><a href="#cb18-79" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-80"><a href="#cb18-80" aria-hidden="true" tabindex="-1"></a><span class="co"># Load data (common to both training and resuming)</span></span>
<span id="cb18-81"><a href="#cb18-81" aria-hidden="true" tabindex="-1"></a>train_x, train_y, test_x, test_y <span class="op">=</span> <span class="fu">load_data</span>();</span>
<span id="cb18-82"><a href="#cb18-82" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-83"><a href="#cb18-83" aria-hidden="true" tabindex="-1"></a><span class="co"># Build the model</span></span>
<span id="cb18-84"><a href="#cb18-84" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> <span class="fu">build_model</span>()</span>
<span id="cb18-85"><a href="#cb18-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-86"><a href="#cb18-86" aria-hidden="true" tabindex="-1"></a><span class="co"># First phase: Train for 5 epochs and save checkpoint</span></span>
<span id="cb18-87"><a href="#cb18-87" aria-hidden="true" tabindex="-1"></a>first_phase_epochs <span class="op">=</span> <span class="fl">5</span></span>
<span id="cb18-88"><a href="#cb18-88" aria-hidden="true" tabindex="-1"></a>model, current_epoch <span class="op">=</span> <span class="fu">train_model!</span>(model, train_x, train_y; epochs<span class="op">=</span>first_phase_epochs, checkpoint_interval<span class="op">=</span>first_phase_epochs)</span>
<span id="cb18-89"><a href="#cb18-89" aria-hidden="true" tabindex="-1"></a>checkpoint_file <span class="op">=</span> <span class="st">"checkpoint_epoch</span><span class="sc">$</span>(current_epoch)<span class="st">.bson"</span></span>
<span id="cb18-90"><a href="#cb18-90" aria-hidden="true" tabindex="-1"></a><span class="fu">println</span>(<span class="st">"Checkpoint saved after training for </span><span class="sc">$</span>(current_epoch)<span class="st"> epoch(s)"</span>)</span>
<span id="cb18-91"><a href="#cb18-91" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-92"><a href="#cb18-92" aria-hidden="true" tabindex="-1"></a><span class="co"># Second phase: Reload checkpoint and retrain</span></span>
<span id="cb18-93"><a href="#cb18-93" aria-hidden="true" tabindex="-1"></a><span class="fu">println</span>(<span class="st">"Reloading checkpoint from file </span><span class="sc">$</span>(checkpoint_file)<span class="st">..."</span>)</span>
<span id="cb18-94"><a href="#cb18-94" aria-hidden="true" tabindex="-1"></a>BSON.<span class="pp">@load</span> checkpoint_file model_params current_epoch</span>
<span id="cb18-95"><a href="#cb18-95" aria-hidden="true" tabindex="-1"></a>model_params <span class="op">=</span> <span class="fu">cu</span>.(model_params);</span>
<span id="cb18-96"><a href="#cb18-96" aria-hidden="true" tabindex="-1"></a>Flux.<span class="fu">loadparams!</span>(model, model_params)</span>
<span id="cb18-97"><a href="#cb18-97" aria-hidden="true" tabindex="-1"></a><span class="fu">println</span>(<span class="st">"Checkpoint reloaded. Resuming training from epoch </span><span class="sc">$</span>(current_epoch <span class="op">+</span> <span class="fl">1</span>)<span class="st">"</span>)</span>
<span id="cb18-98"><a href="#cb18-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-99"><a href="#cb18-99" aria-hidden="true" tabindex="-1"></a><span class="co"># Continue training until total of 10 epochs is reached</span></span>
<span id="cb18-100"><a href="#cb18-100" aria-hidden="true" tabindex="-1"></a>total_epochs <span class="op">=</span> <span class="fl">10</span></span>
<span id="cb18-101"><a href="#cb18-101" aria-hidden="true" tabindex="-1"></a>remaining_epochs <span class="op">=</span> total_epochs <span class="op">-</span> current_epoch</span>
<span id="cb18-102"><a href="#cb18-102" aria-hidden="true" tabindex="-1"></a>model, final_epoch <span class="op">=</span> <span class="fu">train_model!</span>(model, train_x, train_y; epochs<span class="op">=</span>remaining_epochs, checkpoint_interval<span class="op">=</span>remaining_epochs)</span>
<span id="cb18-103"><a href="#cb18-103" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-104"><a href="#cb18-104" aria-hidden="true" tabindex="-1"></a><span class="fu">println</span>(<span class="st">"Training complete at epoch </span><span class="sc">$</span>(current_epoch <span class="op">+</span> final_epoch)<span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p><img src="Retrain.png" alt="Model Checkpoint" style="max-width:100%; width:1000px; height:auto;"></p>
</section>
<section id="mixed-precision-training-float16-on-gpu" class="level2">
<h2 class="anchored" data-anchor-id="mixed-precision-training-float16-on-gpu">Mixed-Precision Training (Float16 on GPU)</h2>
<p>Modern GPUs can achieve higher throughput with lower precision (FP16/BFloat16) using tensor cores.</p>
<p>Flux can leverage this by training with Float16 weights and gradients (mixed precision). This reduces memory usage and can significantly speed up math-intensive models on supported GPUs​.</p>
<p>The main idea is to perform the forward pass and gradient computations in FP16 (for speed) while maintaining an `FP32 “master copy” of the weights.</p>
<p>During updates, gradients are cast to FP32 so that the weight update 𝑤 = 𝑤 − 𝜂 × grad is computed in higher precision.</p>
<section id="the-problem-with-pure-fp16-training" class="level4">
<h4 class="anchored" data-anchor-id="the-problem-with-pure-fp16-training">The Problem with Pure FP16 Training</h4>
<p>When casting both data and model parameters entirely to FP16—especially when also normalizing inputs (e.g.&nbsp;dividing by 255)— risk running into severe precision issues.</p>
<p>FP16 only provides about 3 decimal digits of precision, and small gradient values or scaling operations can cause underflow or rounding errors that accumulate during backpropagation. resulting in <code>NaN</code> or <code>InF</code> Loss.</p>
<pre><code># Convert model parameters to FP16
to_float16(x) = x isa AbstractArray ? Float16.(x) : x
model_f16 = fmap(to_float16, model) |&gt; gpu

# --- Mixed Precision Loss ---
loss_function(x, y) = logitcrossentropy(Float32.(model_f16(x)), Float32.(y))

# --- Optimizer and Training Setup ---
opt = ADAM(0.0001)
parameters = Flux.params(model_f16)
train_loader = DataLoader((train_x_16, train_y_onehot_16), batchsize=128, shuffle=true)

for epoch in 1:5
    for (x_batch, y_batch) in train_loader
        grads = gradient(() -&gt; loss_function(x_batch, y_batch), parameters)
        Flux.Optimise.update!(opt, parameters, grads)
    end
    # Loss monitoring
    subset_x = train_x_16[:, :, :, 1:1000]
    subset_y = train_y_onehot_16[:, 1:1000]
    train_loss = loss_function(subset_x, subset_y)
    println("Epoch $epoch complete. Sample training loss = $train_loss")
end</code></pre>
</section>
<section id="mixed-precision-with-an-fp32-master-copy" class="level4">
<h4 class="anchored" data-anchor-id="mixed-precision-with-an-fp32-master-copy">Mixed Precision with an FP32 Master Copy</h4>
<ol type="1">
<li>Model Initialization:
<ul>
<li>Define your model in FP32.</li>
</ul></li>
<li>Master Copy Creation:
<ul>
<li>Create a deep‑copy of the FP32 parameters (the “master copy”) on the GPU.</li>
</ul></li>
<li>FP16 Computation Model:
<ul>
<li>Convert the model to FP16 (using <code>fmap(to_float16, ...)</code>) for on‑device computation.</li>
</ul></li>
<li>Loss Calculation:
<ul>
<li>Do the forward pass in FP16, then cast outputs (and targets) to FP32 to compute a stable loss.</li>
</ul></li>
<li>Gradient Update:
<ul>
<li>Compute gradients w.r.t. the FP16 model, cast gradients to FP32, update the FP32 master copy using ADAM, and then sync updated FP32 weights back to the FP16 model.</li>
</ul></li>
</ol>
<p><strong>Parameter Storage</strong>:</p>
<ul>
<li><p>Pure FP16: All parameters remain in FP16.</p></li>
<li><p>Mixed Precision: A high-precision FP32 master copy is maintained alongside the FP16 computation model.</p></li>
</ul>
<p><strong>Gradient Update</strong>:</p>
<ul>
<li><p>Pure FP16: Gradients update the FP16 parameters directly.</p></li>
<li><p>Mixed Precision: Gradients are converted to FP32 to update the FP32 master copy, then the FP16 model is synchronized with the updated master parameters.</p></li>
</ul>
<div id="cell-64" class="cell">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Flux</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Flux</span>: Dense, Chain, relu, flatten, gpu, fmap</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Flux.Data</span>: DataLoader</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Flux.Losses</span>: logitcrossentropy</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">MLDatasets</span>, <span class="bu">CUDA</span>, <span class="bu">Plots</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Flux.Optimise</span>: ADAM</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">BSON</span>  </span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>train_x_gpu, train_y_onehot_gpu, test_x_gpu, test_y_onehot_gpu <span class="op">=</span> <span class="fu">load_data</span>();</span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Data Preparation ---</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Convert and reshape training/testing data to Float16 and normalize to [0,1].</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>train_x_16 <span class="op">=</span> <span class="fu">Float16</span>.(<span class="fu">reshape</span>(train_x_gpu, <span class="fl">28</span>, <span class="fl">28</span>, <span class="fl">1</span>, <span class="op">:</span>)); </span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>test_x_16  <span class="op">=</span> <span class="fu">Float16</span>.(<span class="fu">reshape</span>(test_x_gpu, <span class="fl">28</span>, <span class="fl">28</span>, <span class="fl">1</span>, <span class="op">:</span>));</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>train_y_onehot_16 <span class="op">=</span> <span class="fu">Float16</span>.(train_y_onehot_gpu);</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>test_y_onehot_16  <span class="op">=</span> <span class="fu">Float16</span>.(test_y_onehot_gpu);</span>
<span id="cb20-17"><a href="#cb20-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-18"><a href="#cb20-18" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Model Definition (FP32) ---</span></span>
<span id="cb20-19"><a href="#cb20-19" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> <span class="fu">Chain</span>(</span>
<span id="cb20-20"><a href="#cb20-20" aria-hidden="true" tabindex="-1"></a>    flatten,                 <span class="co"># Flatten the input images</span></span>
<span id="cb20-21"><a href="#cb20-21" aria-hidden="true" tabindex="-1"></a>    <span class="fu">Dense</span>(<span class="fl">28</span><span class="op">*</span><span class="fl">28</span>, <span class="fl">128</span>, relu),  <span class="co"># First dense layer with ReLU activation</span></span>
<span id="cb20-22"><a href="#cb20-22" aria-hidden="true" tabindex="-1"></a>    <span class="fu">Dense</span>(<span class="fl">128</span>, <span class="fl">10</span>)           <span class="co"># Output layer (logits for 10 classes)</span></span>
<span id="cb20-23"><a href="#cb20-23" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb20-24"><a href="#cb20-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-25"><a href="#cb20-25" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Create Master Copy (FP32) on GPU ---</span></span>
<span id="cb20-26"><a href="#cb20-26" aria-hidden="true" tabindex="-1"></a><span class="co"># This master copy will be updated using FP32 math.</span></span>
<span id="cb20-27"><a href="#cb20-27" aria-hidden="true" tabindex="-1"></a>master_params <span class="op">=</span> [<span class="fu">deepcopy</span>(p) <span class="op">|&gt;</span> gpu for p <span class="kw">in</span> Flux.<span class="fu">params</span>(model)];</span>
<span id="cb20-28"><a href="#cb20-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-29"><a href="#cb20-29" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Create FP16 Model for On-device Computation ---</span></span>
<span id="cb20-30"><a href="#cb20-30" aria-hidden="true" tabindex="-1"></a><span class="fu">to_float16</span>(x) <span class="op">=</span> x isa <span class="dt">AbstractArray</span> ? <span class="fu">Float16</span>.(x) <span class="op">:</span> x;</span>
<span id="cb20-31"><a href="#cb20-31" aria-hidden="true" tabindex="-1"></a>model_f16 <span class="op">=</span> <span class="fu">fmap</span>(to_float16, model) <span class="op">|&gt;</span> gpu;</span>
<span id="cb20-32"><a href="#cb20-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-33"><a href="#cb20-33" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Mixed Precision Loss ---</span></span>
<span id="cb20-34"><a href="#cb20-34" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute forward pass in FP16 then cast outputs and targets to FP32 for loss computation.</span></span>
<span id="cb20-35"><a href="#cb20-35" aria-hidden="true" tabindex="-1"></a><span class="fu">loss_function</span>(x, y) <span class="op">=</span> <span class="fu">logitcrossentropy</span>(<span class="fu">Float32</span>.(<span class="fu">model_f16</span>(x)), <span class="fu">Float32</span>.(y))</span>
<span id="cb20-36"><a href="#cb20-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-37"><a href="#cb20-37" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Optimizer Setup (for master FP32 parameters) ---</span></span>
<span id="cb20-38"><a href="#cb20-38" aria-hidden="true" tabindex="-1"></a>opt <span class="op">=</span> <span class="fu">ADAM</span>(<span class="fl">0.0001</span>)</span>
<span id="cb20-39"><a href="#cb20-39" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-40"><a href="#cb20-40" aria-hidden="true" tabindex="-1"></a>batch_size <span class="op">=</span> <span class="fl">128</span></span>
<span id="cb20-41"><a href="#cb20-41" aria-hidden="true" tabindex="-1"></a>train_loader <span class="op">=</span> <span class="fu">DataLoader</span>((train_x_16, train_y_onehot_16), batchsize<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="cn">true</span>)</span>
<span id="cb20-42"><a href="#cb20-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-43"><a href="#cb20-43" aria-hidden="true" tabindex="-1"></a>train_losses <span class="op">=</span> <span class="dt">Float32</span>[]</span>
<span id="cb20-44"><a href="#cb20-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-45"><a href="#cb20-45" aria-hidden="true" tabindex="-1"></a>epochs <span class="op">=</span> <span class="fl">10</span></span>
<span id="cb20-46"><a href="#cb20-46" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span>epochs</span>
<span id="cb20-47"><a href="#cb20-47" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (x_batch, y_batch) <span class="kw">in</span> train_loader</span>
<span id="cb20-48"><a href="#cb20-48" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute gradients with respect to the FP16 model parameters.</span></span>
<span id="cb20-49"><a href="#cb20-49" aria-hidden="true" tabindex="-1"></a>        gs <span class="op">=</span> <span class="fu">gradient</span>(() <span class="op">-&gt;</span> <span class="fu">loss_function</span>(x_batch, y_batch), Flux.<span class="fu">params</span>(model_f16))</span>
<span id="cb20-50"><a href="#cb20-50" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb20-51"><a href="#cb20-51" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update each parameter: convert the gradient to FP32 and update the master copy,</span></span>
<span id="cb20-52"><a href="#cb20-52" aria-hidden="true" tabindex="-1"></a>        <span class="co"># then sync the master copy back to the FP16 model.</span></span>
<span id="cb20-53"><a href="#cb20-53" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (p16, p32) <span class="kw">in</span> <span class="fu">zip</span>(Flux.<span class="fu">params</span>(model_f16), master_params)</span>
<span id="cb20-54"><a href="#cb20-54" aria-hidden="true" tabindex="-1"></a>            g <span class="op">=</span> gs[p16]</span>
<span id="cb20-55"><a href="#cb20-55" aria-hidden="true" tabindex="-1"></a>            g32 <span class="op">=</span> <span class="fu">Float32</span>.(g)  <span class="co"># Convert gradient to FP32</span></span>
<span id="cb20-56"><a href="#cb20-56" aria-hidden="true" tabindex="-1"></a>            Flux.Optimise.<span class="fu">update!</span>(opt, p32, g32)</span>
<span id="cb20-57"><a href="#cb20-57" aria-hidden="true" tabindex="-1"></a>            p16 <span class="op">.=</span> <span class="fu">Float16</span>.(p32)  <span class="co"># Sync updated FP32 master to FP16 model</span></span>
<span id="cb20-58"><a href="#cb20-58" aria-hidden="true" tabindex="-1"></a>        <span class="cf">end</span></span>
<span id="cb20-59"><a href="#cb20-59" aria-hidden="true" tabindex="-1"></a>    <span class="cf">end</span></span>
<span id="cb20-60"><a href="#cb20-60" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-61"><a href="#cb20-61" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Evaluate loss on a subset for monitoring.</span></span>
<span id="cb20-62"><a href="#cb20-62" aria-hidden="true" tabindex="-1"></a>    subset_x <span class="op">=</span> train_x_16[<span class="op">:</span>, <span class="op">:</span>, <span class="op">:</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">1000</span>]</span>
<span id="cb20-63"><a href="#cb20-63" aria-hidden="true" tabindex="-1"></a>    subset_y <span class="op">=</span> train_y_onehot_16[<span class="op">:</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">1000</span>]</span>
<span id="cb20-64"><a href="#cb20-64" aria-hidden="true" tabindex="-1"></a>    train_loss <span class="op">=</span> <span class="fu">loss_function</span>(subset_x, subset_y)</span>
<span id="cb20-65"><a href="#cb20-65" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb20-66"><a href="#cb20-66" aria-hidden="true" tabindex="-1"></a>    <span class="fu">push!</span>(train_losses, <span class="fu">Float32</span>(train_loss))</span>
<span id="cb20-67"><a href="#cb20-67" aria-hidden="true" tabindex="-1"></a>    <span class="fu">println</span>(<span class="st">"Epoch </span><span class="sc">$</span>epoch<span class="st"> complete. Sample training loss = </span><span class="sc">$</span>train_loss<span class="st">"</span>)</span>
<span id="cb20-68"><a href="#cb20-68" aria-hidden="true" tabindex="-1"></a><span class="cf">end</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div style="display: flex; justify-content: space-between;">
<div style="flex: 1; margin-right: 20px;">
<p><strong>Proper loss with learning FP32 Copy</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Epoch</th>
<th>Loss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Epoch 1 complete.</td>
<td>Sample training loss = 0.504</td>
</tr>
<tr class="even">
<td>Epoch 2 complete.</td>
<td>Sample training loss = 0.363</td>
</tr>
<tr class="odd">
<td>Epoch 3 complete.</td>
<td>Sample training loss = 0.311</td>
</tr>
<tr class="even">
<td>Epoch 4 complete.</td>
<td>Sample training loss = 0.274</td>
</tr>
<tr class="odd">
<td>Epoch 5 complete.</td>
<td>Sample training loss = 0.253</td>
</tr>
</tbody>
</table>
</div>
<div style="flex: 1; margin-left: 30px;">
<p><strong>Unstable loss with learning Everything in FP16</strong></p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Epoch</th>
<th>Loss</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Epoch 1 complete.</td>
<td>Sample training loss = NaN</td>
</tr>
<tr class="even">
<td>Epoch 2 complete.</td>
<td>Sample training loss = NaN</td>
</tr>
<tr class="odd">
<td>Epoch 3 complete.</td>
<td>Sample training loss = NaN</td>
</tr>
<tr class="even">
<td>Epoch 4 complete.</td>
<td>Sample training loss = NaN</td>
</tr>
<tr class="odd">
<td>Epoch 5 complete.</td>
<td>Sample training loss = NaN</td>
</tr>
</tbody>
</table>
</div>
</div>
<p>Things to consider</p>
<ol type="1">
<li><p><a href="https://computing.stat.berkeley.edu/tutorial-parallelization/parallel-python#6-using-the-gpu-via-pytorch">The GPU memory is separate from CPU memory</a>, and transferring data from the CPU to GPU (or back) is often more costly than doing the computation on the GPU.</p></li>
<li><p>On the other hand, GPUs can process these lower-precision operations (FP16) faster—sometimes leveraging specialized hardware like <a href="https://datacrunch.io/blog/role-of-tensor-cores-in-parallel-computing-and-ai">Tensor Cores</a> (Processing unit in A100, H100 etc) —which results in a significant speed-up.</p></li>
</ol>
</section>
<section id="memory-speed" class="level4">
<h4 class="anchored" data-anchor-id="memory-speed">Memory &amp; Speed</h4>
<div id="cell-68" class="cell">
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">CUDA</span>, <span class="bu">BenchmarkTools</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>x_cpu_16 <span class="op">=</span> <span class="fu">rand</span>(<span class="dt">Float16</span>, <span class="fl">28</span>, <span class="fl">28</span>, <span class="fl">1</span>, <span class="fl">1000</span>);</span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a>x_cpu_32 <span class="op">=</span> <span class="fu">rand</span>(<span class="dt">Float32</span>, <span class="fl">28</span>, <span class="fl">28</span>, <span class="fl">1</span>, <span class="fl">1000</span>);</span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a><span class="co"># CPU -&gt; GPU transfer time for Float16:</span></span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a><span class="pp">@btime</span> CUDA.<span class="pp">@sync</span> <span class="cf">begin</span></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a>    x_gpu_16 <span class="op">=</span> <span class="fu">CuArray</span>(x_cpu_16)</span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>    <span class="cn">nothing</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a><span class="cf">end</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a><span class="co"># 350.586 μs (10 allocations: 304 bytes)</span></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a><span class="co"># CPU -&gt; GPU transfer time for Float32</span></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a><span class="pp">@btime</span> CUDA.<span class="pp">@sync</span> <span class="cf">begin</span></span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>    x_gpu_32 <span class="op">=</span> <span class="fu">CuArray</span>(x_cpu_32)</span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>    <span class="cn">nothing</span></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a><span class="cf">end</span></span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a><span class="co"># 690.255 μs (10 allocations: 304 bytes)</span></span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>x_gpu_16 <span class="op">=</span> <span class="fu">CuArray</span>(x_cpu_16);</span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>x_gpu_32 <span class="op">=</span> <span class="fu">CuArray</span>(x_cpu_32);</span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a><span class="co"># GPU -&gt; CPU transfer time for Float16</span></span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a><span class="pp">@btime</span> CUDA.<span class="pp">@sync</span> <span class="cf">begin</span></span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>    out_16 <span class="op">=</span> <span class="fu">Array</span>(x_gpu_16)</span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>    <span class="cn">nothing</span></span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a><span class="cf">end</span></span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a><span class="co"># 391.073 μs (8 allocations: 1.50 MiB)</span></span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a><span class="co"># GPU -&gt; CPU transfer time for Float32</span></span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a><span class="pp">@btime</span> CUDA.<span class="pp">@sync</span> <span class="cf">begin</span></span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a>    out_32 <span class="op">=</span> <span class="fu">Array</span>(x_gpu_32)</span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"></a>    <span class="cn">nothing</span></span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true" tabindex="-1"></a><span class="cf">end</span></span>
<span id="cb21-36"><a href="#cb21-36" aria-hidden="true" tabindex="-1"></a><span class="co"># 686.994 μs (8 allocations: 2.99 MiB)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="tensor-core-speed-up" class="level4">
<h4 class="anchored" data-anchor-id="tensor-core-speed-up">Tensor Core Speed Up</h4>
<div id="cell-70" class="cell">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Modified from Note 8: GPUs</span></span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>n <span class="op">=</span> <span class="fl">1000</span></span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>x16_gpu <span class="op">=</span> CUDA.<span class="fu">randn</span>(<span class="dt">Float16</span>, n, n);</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>y16_gpu <span class="op">=</span> CUDA.<span class="fu">randn</span>(<span class="dt">Float16</span>, n, n);</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>x32_gpu <span class="op">=</span> CUDA.<span class="fu">randn</span>(n, n); </span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>y32_gpu <span class="op">=</span> CUDA.<span class="fu">randn</span>(n, n);</span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">matmult</span>(x, y)</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>    z <span class="op">=</span> x <span class="op">*</span> y;</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> z</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a><span class="pp">@btime</span> CUDA.<span class="pp">@sync</span> z16_gpu <span class="op">=</span> <span class="fu">matmult</span>(x16_gpu, y16_gpu);</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a><span class="co"># 43.978 μs (50 allocations: 1.17 KiB)</span></span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a><span class="pp">@btime</span> CUDA.<span class="pp">@sync</span> z32_gpu <span class="op">=</span> <span class="fu">matmult</span>(x32_gpu, y32_gpu);</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a><span class="co"># 157.912 μs (50 allocations: 1.17 KiB)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<ul>
<li><p>Float32 values use 4 bytes per element versus 2 bytes per element for Float16. Hence, transferring a Float32 array 2x data, resulting 2x time.</p></li>
<li><p>304 bytes is the overhead of CPU -&gt; GPU transfer?</p></li>
<li><p>The allocation size (1.50 MiB vs.&nbsp;2.99 MiB) GPU -&gt; CPU corresponds to the actual data size in memory</p></li>
<li><p>Close to 4x speed up in matrix multiplication hint the contribution of tensor cores</p></li>
</ul>
</section>
<section id="gradient-scaling-optional" class="level4">
<h4 class="anchored" data-anchor-id="gradient-scaling-optional">Gradient Scaling (Optional)</h4>
<p>In some cases, especially when the loss is very small, you might apply a loss scaling factor to prevent gradient underflow in FP16.</p>
<ol type="1">
<li><strong>Add a loss scaling factor</strong> which multiply the loss before backpropagation</li>
<li><strong>Modify loss function</strong> to return both scaled loss (backpropagation) and unscaled loss (monitoring)<br>
</li>
<li><strong>Updatethe master model weight</strong> in unscaled FP32 precision and copy it back to FP16</li>
</ol>
<div id="cell-74" class="cell">
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Mixed Precision Loss ---</span></span>
<span id="cb23-2"><a href="#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute forward pass in FP16 then cast outputs and targets to FP32 for loss computation.</span></span>
<span id="cb23-3"><a href="#cb23-3" aria-hidden="true" tabindex="-1"></a><span class="fu">loss_function</span>(x, y) <span class="op">=</span> <span class="fu">logitcrossentropy</span>(<span class="fu">Float32</span>.(<span class="fu">model_f16</span>(x)), <span class="fu">Float32</span>.(y))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Change to</p>
<div id="cell-76" class="cell">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="co"># --- Mixed Precision Loss with Gradient Scaling ---</span></span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>loss_scale <span class="op">=</span> <span class="fu">Float32</span>(<span class="fl">128.0</span>)  <span class="co"># loss scaling factor</span></span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Compute forward pass in FP16, apply loss scaling, and cast to FP32 for stable loss computation</span></span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">loss_function</span>(x, y)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>    outputs <span class="op">=</span> <span class="fu">model_f16</span>(x) <span class="co"># Forward pass in FP16</span></span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a>    loss <span class="op">=</span> <span class="fu">logitcrossentropy</span>(<span class="fu">Float32</span>.(outputs), <span class="fu">Float32</span>.(y)) <span class="co"># FP32 loss computation</span></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>    scaled_loss <span class="op">=</span> loss <span class="op">*</span> loss_scale <span class="co"># Scale loss </span></span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> scaled_loss, loss  <span class="co"># Return both scaled and unscaled loss</span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>And</p>
<div id="cell-78" class="cell">
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span>epochs</span>
<span id="cb25-2"><a href="#cb25-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (x_batch, y_batch) <span class="kw">in</span> train_loader</span>
<span id="cb25-3"><a href="#cb25-3" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Gradients w.r.t FP16 model Parms.</span></span>
<span id="cb25-4"><a href="#cb25-4" aria-hidden="true" tabindex="-1"></a>        gs <span class="op">=</span> <span class="fu">gradient</span>(() <span class="op">-&gt;</span> <span class="fu">loss_function</span>(x_batch, y_batch), Flux.<span class="fu">params</span>(model_f16))</span>
<span id="cb25-5"><a href="#cb25-5" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb25-6"><a href="#cb25-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb25-7"><a href="#cb25-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (p16, p32) <span class="kw">in</span> <span class="fu">zip</span>(Flux.<span class="fu">params</span>(model_f16), master_params)</span>
<span id="cb25-8"><a href="#cb25-8" aria-hidden="true" tabindex="-1"></a>            g <span class="op">=</span> gs[p16]</span>
<span id="cb25-9"><a href="#cb25-9" aria-hidden="true" tabindex="-1"></a>            g32 <span class="op">=</span> <span class="fu">Float32</span>.(g)  <span class="co"># Convert gradient to FP32</span></span>
<span id="cb25-10"><a href="#cb25-10" aria-hidden="true" tabindex="-1"></a>            Flux.Optimise.<span class="fu">update!</span>(opt, p32, g32)</span>
<span id="cb25-11"><a href="#cb25-11" aria-hidden="true" tabindex="-1"></a>            p16 <span class="op">.=</span> <span class="fu">Float16</span>.(p32)  <span class="co"># Sync updated FP32 master to FP16 model</span></span>
<span id="cb25-12"><a href="#cb25-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">end</span></span>
<span id="cb25-13"><a href="#cb25-13" aria-hidden="true" tabindex="-1"></a>    <span class="cf">end</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Change to</p>
<div id="cell-80" class="cell">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> epoch <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span>epochs</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> (x_batch, y_batch) <span class="kw">in</span> train_loader</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Use the scaled loss for backpropagation</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>        gs <span class="op">=</span> <span class="fu">gradient</span>(Flux.<span class="fu">params</span>(model_f16)) <span class="cf">do</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>            scaled_loss, _ <span class="op">=</span> <span class="fu">loss_function</span>(x_batch, y_batch)</span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> scaled_loss</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">end</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (p16, p32) <span class="kw">in</span> <span class="fu">zip</span>(Flux.<span class="fu">params</span>(model_f16), master_params)</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>            g <span class="op">=</span> gs[p16]</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>            g32 <span class="op">=</span> <span class="fu">Float32</span>.(g) <span class="co"># Convert gradient to FP32</span></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>            g32 <span class="op">./=</span> loss_scale <span class="co"># Unscale the gradient</span></span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>            Flux.Optimise.<span class="fu">update!</span>(opt, p32, g32) <span class="co"># Sync updated FP32 master (unscaled gradient)</span></span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>            p16 <span class="op">.=</span> <span class="fu">Float16</span>.(p32)</span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">end</span></span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">end</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="references" class="level3">
<h3 class="anchored" data-anchor-id="references">References</h3>
<ul>
<li><p><a href="https://www.dropbox.com/scl/fi/xk8bc4l68oyuxhbzsnrwh/Case-Mixed-Precision-Training-NeurIPS-Expo.pdf?rlkey=jee9fp9mkus03pinbb6xj5kqz&amp;e=3&amp;dl=0">NVIDIA Mixed Precision Training Guidelines</a></p></li>
<li><p><a href="https://forums.fast.ai/t/mixed-precision-training/20720/1">Mixed Precision Training Conceptual Understanding</a></p></li>
</ul>
</section>
</section>
<section id="comparison-between-flux.jl-and-pytorch" class="level2">
<h2 class="anchored" data-anchor-id="comparison-between-flux.jl-and-pytorch">Comparison Between Flux.jl and PyTorch</h2>
<table class="caption-top table">
<colgroup>
<col style="width: 23%">
<col style="width: 33%">
<col style="width: 43%">
</colgroup>
<thead>
<tr class="header">
<th>Feature</th>
<th>Flux (Julia)</th>
<th>PyTorch (Python)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Language</td>
<td>Pure Julia, JIT compiled</td>
<td>Python, with C++ backend</td>
</tr>
<tr class="even">
<td>GPU Support</td>
<td>Native via CUDA.jl</td>
<td>Native via CUDA</td>
</tr>
<tr class="odd">
<td>Ease of Use</td>
<td>Mathematical notation, hackable, and seamless integration with custom code</td>
<td>Flexible with extensive documentation and plug-and-play training loops</td>
</tr>
<tr class="even">
<td>Ecosystem</td>
<td>Growing; integrates with SciML and specialized libraries (e.g., DiffEqFlux)</td>
<td>Mature with a large community and extensive model/tool libraries</td>
</tr>
<tr class="odd">
<td>Performance</td>
<td>High performance after JIT warm-up; may incur overhead on many small ops</td>
<td>Highly optimized with fused kernels and efficient memory management</td>
</tr>
</tbody>
</table>
</section>
<section id="appendix" class="level2">
<h2 class="anchored" data-anchor-id="appendix">Appendix</h2>
<section id="gpu-kernels-for-activation-function" class="level4">
<h4 class="anchored" data-anchor-id="gpu-kernels-for-activation-function">GPU Kernels for Activation Function</h4>
<div id="cell-86" class="cell">
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Adpated from Note 8: GPUs</span></span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">CUDA</span>, <span class="bu">BenchmarkTools</span></span>
<span id="cb27-3"><a href="#cb27-3" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">Flux</span>: flatten</span>
<span id="cb27-4"><a href="#cb27-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-5"><a href="#cb27-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Custom Kernel ReLU Activation</span></span>
<span id="cb27-6"><a href="#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">relu_kernel!</span>(x)</span>
<span id="cb27-7"><a href="#cb27-7" aria-hidden="true" tabindex="-1"></a>    i <span class="op">=</span> (<span class="fu">blockIdx</span>().x <span class="op">-</span> <span class="fl">1</span>) <span class="op">*</span> <span class="fu">blockDim</span>().x <span class="op">+</span> <span class="fu">threadIdx</span>().x</span>
<span id="cb27-8"><a href="#cb27-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&lt;=</span> <span class="fu">length</span>(x)</span>
<span id="cb27-9"><a href="#cb27-9" aria-hidden="true" tabindex="-1"></a>        x[i] <span class="op">=</span> x[i] <span class="op">&gt;</span> <span class="fl">0f0</span> ? x[i] <span class="op">:</span> <span class="fl">0f0</span></span>
<span id="cb27-10"><a href="#cb27-10" aria-hidden="true" tabindex="-1"></a>    <span class="cf">end</span></span>
<span id="cb27-11"><a href="#cb27-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span></span>
<span id="cb27-12"><a href="#cb27-12" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb27-13"><a href="#cb27-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-14"><a href="#cb27-14" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">relu_activation!</span>(x) <span class="co"># Helper function</span></span>
<span id="cb27-15"><a href="#cb27-15" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="fu">length</span>(x)</span>
<span id="cb27-16"><a href="#cb27-16" aria-hidden="true" tabindex="-1"></a>    threads <span class="op">=</span> <span class="fu">min</span>(n, <span class="fl">1024</span>)</span>
<span id="cb27-17"><a href="#cb27-17" aria-hidden="true" tabindex="-1"></a>    blocks <span class="op">=</span> <span class="fu">cld</span>(n, threads)</span>
<span id="cb27-18"><a href="#cb27-18" aria-hidden="true" tabindex="-1"></a>    <span class="pp">@cuda</span> threads<span class="op">=</span>threads blocks<span class="op">=</span>blocks <span class="fu">relu_kernel!</span>(x)</span>
<span id="cb27-19"><a href="#cb27-19" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span></span>
<span id="cb27-20"><a href="#cb27-20" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb27-21"><a href="#cb27-21" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-22"><a href="#cb27-22" aria-hidden="true" tabindex="-1"></a><span class="co"># Baseline: Broadcasting-based ReLU</span></span>
<span id="cb27-23"><a href="#cb27-23" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">relu_builtin</span>(x)</span>
<span id="cb27-24"><a href="#cb27-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fu">max</span>.(x, <span class="fl">0f0</span>)</span>
<span id="cb27-25"><a href="#cb27-25" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb27-26"><a href="#cb27-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-27"><a href="#cb27-27" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> <span class="fu">cu</span>(<span class="fu">randn</span>(<span class="dt">Float32</span>, <span class="fl">28</span>, <span class="fl">28</span>, <span class="fl">1</span>, <span class="fl">128</span>));</span>
<span id="cb27-28"><a href="#cb27-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-29"><a href="#cb27-29" aria-hidden="true" tabindex="-1"></a><span class="co"># Custom Kernel</span></span>
<span id="cb27-30"><a href="#cb27-30" aria-hidden="true" tabindex="-1"></a><span class="pp">@btime</span> CUDA.<span class="pp">@sync</span> <span class="cf">begin</span></span>
<span id="cb27-31"><a href="#cb27-31" aria-hidden="true" tabindex="-1"></a>    <span class="fu">relu_activation!</span>(x)</span>
<span id="cb27-32"><a href="#cb27-32" aria-hidden="true" tabindex="-1"></a>    <span class="cn">nothing</span></span>
<span id="cb27-33"><a href="#cb27-33" aria-hidden="true" tabindex="-1"></a><span class="cf">end</span></span>
<span id="cb27-34"><a href="#cb27-34" aria-hidden="true" tabindex="-1"></a><span class="co"># 14.538 μs (18 allocations: 560 bytes)</span></span>
<span id="cb27-35"><a href="#cb27-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb27-36"><a href="#cb27-36" aria-hidden="true" tabindex="-1"></a><span class="co"># Baseline</span></span>
<span id="cb27-37"><a href="#cb27-37" aria-hidden="true" tabindex="-1"></a><span class="pp">@btime</span> CUDA.<span class="pp">@sync</span> <span class="cf">begin</span></span>
<span id="cb27-38"><a href="#cb27-38" aria-hidden="true" tabindex="-1"></a>    y_builtin <span class="op">=</span> <span class="fu">relu_builtin</span>(<span class="op">$</span>x)</span>
<span id="cb27-39"><a href="#cb27-39" aria-hidden="true" tabindex="-1"></a>    <span class="cn">nothing</span></span>
<span id="cb27-40"><a href="#cb27-40" aria-hidden="true" tabindex="-1"></a><span class="cf">end</span></span>
<span id="cb27-41"><a href="#cb27-41" aria-hidden="true" tabindex="-1"></a><span class="co"># 26.857 μs (70 allocations: 1.72 KiB)</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div id="cell-87" class="cell">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode julia code-with-copy"><code class="sourceCode julia"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a><span class="im">using</span> <span class="bu">CUDA</span>, <span class="bu">BenchmarkTools</span>, <span class="bu">Flux</span>, <span class="bu">MLDatasets</span>, <span class="bu">BSON</span>, <span class="bu">Plots</span>, <span class="bu">Zygote</span></span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-3"><a href="#cb28-3" aria-hidden="true" tabindex="-1"></a><span class="co"># ---------------------------------------------------------------------</span></span>
<span id="cb28-4"><a href="#cb28-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Custom Kernel ReLU Activation (in-place version for GPU arrays)</span></span>
<span id="cb28-5"><a href="#cb28-5" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">simple_relu_kernel!</span>(x)</span>
<span id="cb28-6"><a href="#cb28-6" aria-hidden="true" tabindex="-1"></a>    i <span class="op">=</span> (<span class="fu">blockIdx</span>().x <span class="op">-</span> <span class="fl">1</span>) <span class="op">*</span> <span class="fu">blockDim</span>().x <span class="op">+</span> <span class="fu">threadIdx</span>().x</span>
<span id="cb28-7"><a href="#cb28-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> i <span class="op">&lt;=</span> <span class="fu">length</span>(x)</span>
<span id="cb28-8"><a href="#cb28-8" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> x[i] <span class="op">&lt;</span> <span class="fl">0f0</span></span>
<span id="cb28-9"><a href="#cb28-9" aria-hidden="true" tabindex="-1"></a>            x[i] <span class="op">=</span> <span class="fl">0f0</span></span>
<span id="cb28-10"><a href="#cb28-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">end</span></span>
<span id="cb28-11"><a href="#cb28-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">end</span></span>
<span id="cb28-12"><a href="#cb28-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span></span>
<span id="cb28-13"><a href="#cb28-13" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb28-14"><a href="#cb28-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-15"><a href="#cb28-15" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">simple_relu_activation!</span>(x)</span>
<span id="cb28-16"><a href="#cb28-16" aria-hidden="true" tabindex="-1"></a>    n <span class="op">=</span> <span class="fu">length</span>(x)</span>
<span id="cb28-17"><a href="#cb28-17" aria-hidden="true" tabindex="-1"></a>    threads <span class="op">=</span> <span class="fu">min</span>(n, <span class="fl">1024</span>)</span>
<span id="cb28-18"><a href="#cb28-18" aria-hidden="true" tabindex="-1"></a>    blocks <span class="op">=</span> <span class="fu">cld</span>(n, threads)</span>
<span id="cb28-19"><a href="#cb28-19" aria-hidden="true" tabindex="-1"></a>    <span class="pp">@cuda</span> threads<span class="op">=</span>threads blocks<span class="op">=</span>blocks <span class="fu">simple_relu_kernel!</span>(x)</span>
<span id="cb28-20"><a href="#cb28-20" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span></span>
<span id="cb28-21"><a href="#cb28-21" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb28-22"><a href="#cb28-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-23"><a href="#cb28-23" aria-hidden="true" tabindex="-1"></a><span class="co"># ---------------------------------------------------------------------</span></span>
<span id="cb28-24"><a href="#cb28-24" aria-hidden="true" tabindex="-1"></a><span class="co"># Custom ReLU function that copies the input (to keep AD safe) and applies the kernel</span></span>
<span id="cb28-25"><a href="#cb28-25" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">custom_relu</span>(x)</span>
<span id="cb28-26"><a href="#cb28-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> x isa CUDA.CuArray</span>
<span id="cb28-27"><a href="#cb28-27" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> <span class="fu">similar</span>(x)</span>
<span id="cb28-28"><a href="#cb28-28" aria-hidden="true" tabindex="-1"></a>        <span class="fu">copy!</span>(y, x)</span>
<span id="cb28-29"><a href="#cb28-29" aria-hidden="true" tabindex="-1"></a>        <span class="fu">simple_relu_activation!</span>(y)</span>
<span id="cb28-30"><a href="#cb28-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> y</span>
<span id="cb28-31"><a href="#cb28-31" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span></span>
<span id="cb28-32"><a href="#cb28-32" aria-hidden="true" tabindex="-1"></a>        <span class="pp">@info</span> <span class="st">"Using CPU-based ReLU"</span></span>
<span id="cb28-33"><a href="#cb28-33" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="fu">max</span>.(x, <span class="fl">0f0</span>)</span>
<span id="cb28-34"><a href="#cb28-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">end</span></span>
<span id="cb28-35"><a href="#cb28-35" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb28-36"><a href="#cb28-36" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-37"><a href="#cb28-37" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a custom adjoint for custom_relu so Zygote doesn't try to differentiate through our GPU call.</span></span>
<span id="cb28-38"><a href="#cb28-38" aria-hidden="true" tabindex="-1"></a>Zygote.<span class="pp">@adjoint</span> <span class="kw">function</span> <span class="fu">custom_relu</span>(x)</span>
<span id="cb28-39"><a href="#cb28-39" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> <span class="fu">custom_relu</span>(x)</span>
<span id="cb28-40"><a href="#cb28-40" aria-hidden="true" tabindex="-1"></a>    <span class="kw">function</span> <span class="fu">pullback</span>(Δ)</span>
<span id="cb28-41"><a href="#cb28-41" aria-hidden="true" tabindex="-1"></a>        <span class="co"># The derivative of ReLU is 1 for x &gt; 0, else 0.</span></span>
<span id="cb28-42"><a href="#cb28-42" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> (Δ <span class="op">.*</span> (x <span class="op">.&gt;</span> <span class="fl">0f0</span>),)</span>
<span id="cb28-43"><a href="#cb28-43" aria-hidden="true" tabindex="-1"></a>    <span class="kw">end</span></span>
<span id="cb28-44"><a href="#cb28-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> y, pullback</span>
<span id="cb28-45"><a href="#cb28-45" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb28-46"><a href="#cb28-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-47"><a href="#cb28-47" aria-hidden="true" tabindex="-1"></a><span class="co"># ---------------------------------------------------------------------</span></span>
<span id="cb28-48"><a href="#cb28-48" aria-hidden="true" tabindex="-1"></a><span class="co"># Define a custom layer that applies our custom_relu</span></span>
<span id="cb28-49"><a href="#cb28-49" aria-hidden="true" tabindex="-1"></a><span class="kw">struct</span> CustomReLULayer</span>
<span id="cb28-50"><a href="#cb28-50" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb28-51"><a href="#cb28-51" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-52"><a href="#cb28-52" aria-hidden="true" tabindex="-1"></a>(c<span class="op">::</span><span class="dt">CustomReLULayer</span>)(x) <span class="op">=</span> <span class="fu">custom_relu</span>(x)</span>
<span id="cb28-53"><a href="#cb28-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-54"><a href="#cb28-54" aria-hidden="true" tabindex="-1"></a><span class="co"># ---------------------------------------------------------------------</span></span>
<span id="cb28-55"><a href="#cb28-55" aria-hidden="true" tabindex="-1"></a><span class="co"># Utility: move data to GPU if available</span></span>
<span id="cb28-56"><a href="#cb28-56" aria-hidden="true" tabindex="-1"></a><span class="fu">device</span>(x) <span class="op">=</span> CUDA.<span class="fu">has_cuda</span>() ? <span class="fu">cu</span>(x) <span class="op">:</span> x</span>
<span id="cb28-57"><a href="#cb28-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-58"><a href="#cb28-58" aria-hidden="true" tabindex="-1"></a><span class="co"># Data loading and preprocessing</span></span>
<span id="cb28-59"><a href="#cb28-59" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">load_data</span>()</span>
<span id="cb28-60"><a href="#cb28-60" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Load MNIST data</span></span>
<span id="cb28-61"><a href="#cb28-61" aria-hidden="true" tabindex="-1"></a>    train_x, train_y <span class="op">=</span> MLDatasets.MNIST.<span class="fu">traindata</span>()</span>
<span id="cb28-62"><a href="#cb28-62" aria-hidden="true" tabindex="-1"></a>    test_x,  test_y  <span class="op">=</span> MLDatasets.MNIST.<span class="fu">testdata</span>()</span>
<span id="cb28-63"><a href="#cb28-63" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-64"><a href="#cb28-64" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Convert images to Float32 and send to device (GPU if available)</span></span>
<span id="cb28-65"><a href="#cb28-65" aria-hidden="true" tabindex="-1"></a>    train_x <span class="op">=</span> <span class="fu">device</span>(<span class="fu">Float32</span>.(train_x))</span>
<span id="cb28-66"><a href="#cb28-66" aria-hidden="true" tabindex="-1"></a>    test_x  <span class="op">=</span> <span class="fu">device</span>(<span class="fu">Float32</span>.(test_x))</span>
<span id="cb28-67"><a href="#cb28-67" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-68"><a href="#cb28-68" aria-hidden="true" tabindex="-1"></a>    <span class="co"># One-hot encode labels and send to device</span></span>
<span id="cb28-69"><a href="#cb28-69" aria-hidden="true" tabindex="-1"></a>    train_y_onehot <span class="op">=</span> <span class="fu">device</span>(Flux.<span class="fu">onehotbatch</span>(train_y, <span class="fl">0</span><span class="op">:</span><span class="fl">9</span>))</span>
<span id="cb28-70"><a href="#cb28-70" aria-hidden="true" tabindex="-1"></a>    test_y_onehot  <span class="op">=</span> <span class="fu">device</span>(Flux.<span class="fu">onehotbatch</span>(test_y, <span class="fl">0</span><span class="op">:</span><span class="fl">9</span>))</span>
<span id="cb28-71"><a href="#cb28-71" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-72"><a href="#cb28-72" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> train_x, train_y_onehot, test_x, test_y_onehot</span>
<span id="cb28-73"><a href="#cb28-73" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb28-74"><a href="#cb28-74" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-75"><a href="#cb28-75" aria-hidden="true" tabindex="-1"></a><span class="co"># Build the model using the custom ReLU layer</span></span>
<span id="cb28-76"><a href="#cb28-76" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">build_model</span>()</span>
<span id="cb28-77"><a href="#cb28-77" aria-hidden="true" tabindex="-1"></a>    model <span class="op">=</span> <span class="fu">Chain</span>(</span>
<span id="cb28-78"><a href="#cb28-78" aria-hidden="true" tabindex="-1"></a>        Flux.flatten,</span>
<span id="cb28-79"><a href="#cb28-79" aria-hidden="true" tabindex="-1"></a>        <span class="fu">Dense</span>(<span class="fl">28</span><span class="op">*</span><span class="fl">28</span>, <span class="fl">128</span>),  <span class="co"># no activation here</span></span>
<span id="cb28-80"><a href="#cb28-80" aria-hidden="true" tabindex="-1"></a>        <span class="fu">CustomReLULayer</span>(),    <span class="co"># apply our custom activation as a separate layer</span></span>
<span id="cb28-81"><a href="#cb28-81" aria-hidden="true" tabindex="-1"></a>        <span class="fu">Dense</span>(<span class="fl">128</span>, <span class="fl">10</span>)</span>
<span id="cb28-82"><a href="#cb28-82" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb28-83"><a href="#cb28-83" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="fu">device</span>(model)</span>
<span id="cb28-84"><a href="#cb28-84" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb28-85"><a href="#cb28-85" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-86"><a href="#cb28-86" aria-hidden="true" tabindex="-1"></a><span class="co"># ---------------------------------------------------------------------</span></span>
<span id="cb28-87"><a href="#cb28-87" aria-hidden="true" tabindex="-1"></a><span class="co"># Define the loss function</span></span>
<span id="cb28-88"><a href="#cb28-88" aria-hidden="true" tabindex="-1"></a><span class="fu">loss_function</span>(model, x, y) <span class="op">=</span> Flux.Losses.<span class="fu">logitcrossentropy</span>(<span class="fu">model</span>(x), y)</span>
<span id="cb28-89"><a href="#cb28-89" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-90"><a href="#cb28-90" aria-hidden="true" tabindex="-1"></a><span class="co"># Training loop with optional checkpointing</span></span>
<span id="cb28-91"><a href="#cb28-91" aria-hidden="true" tabindex="-1"></a><span class="kw">function</span> <span class="fu">train_model!</span>(model, train_x, train_y; </span>
<span id="cb28-92"><a href="#cb28-92" aria-hidden="true" tabindex="-1"></a>                      epochs<span class="op">=</span><span class="fl">5</span>, batch_size<span class="op">=</span><span class="fl">128</span>, </span>
<span id="cb28-93"><a href="#cb28-93" aria-hidden="true" tabindex="-1"></a>                      checkpoint_interval<span class="op">=</span><span class="fl">5</span>, checkpoint_prefix<span class="op">=</span><span class="st">"checkpoint_epoch"</span>)</span>
<span id="cb28-94"><a href="#cb28-94" aria-hidden="true" tabindex="-1"></a>    opt <span class="op">=</span> Flux.Optimise.<span class="fu">ADAM</span>(<span class="fl">0.0001</span>)</span>
<span id="cb28-95"><a href="#cb28-95" aria-hidden="true" tabindex="-1"></a>    params <span class="op">=</span> Flux.<span class="fu">params</span>(model)</span>
<span id="cb28-96"><a href="#cb28-96" aria-hidden="true" tabindex="-1"></a>    train_loader <span class="op">=</span> Flux.Data.<span class="fu">DataLoader</span>((train_x, train_y), batchsize<span class="op">=</span>batch_size, shuffle<span class="op">=</span><span class="cn">true</span>)</span>
<span id="cb28-97"><a href="#cb28-97" aria-hidden="true" tabindex="-1"></a>    current_epoch <span class="op">=</span> <span class="fl">0</span></span>
<span id="cb28-98"><a href="#cb28-98" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-99"><a href="#cb28-99" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> epoch <span class="kw">in</span> <span class="fl">1</span><span class="op">:</span>epochs</span>
<span id="cb28-100"><a href="#cb28-100" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> (x_batch, y_batch) <span class="kw">in</span> train_loader</span>
<span id="cb28-101"><a href="#cb28-101" aria-hidden="true" tabindex="-1"></a>            grads <span class="op">=</span> Flux.<span class="fu">gradient</span>(() <span class="op">-&gt;</span> <span class="fu">loss_function</span>(model, x_batch, y_batch), params)</span>
<span id="cb28-102"><a href="#cb28-102" aria-hidden="true" tabindex="-1"></a>            Flux.Optimise.<span class="fu">update!</span>(opt, params, grads)</span>
<span id="cb28-103"><a href="#cb28-103" aria-hidden="true" tabindex="-1"></a>        <span class="cf">end</span></span>
<span id="cb28-104"><a href="#cb28-104" aria-hidden="true" tabindex="-1"></a>        current_epoch <span class="op">+=</span> <span class="fl">1</span></span>
<span id="cb28-105"><a href="#cb28-105" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-106"><a href="#cb28-106" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute a sample loss on a fixed subset</span></span>
<span id="cb28-107"><a href="#cb28-107" aria-hidden="true" tabindex="-1"></a>        subset_x <span class="op">=</span> train_x[<span class="op">:</span>, <span class="op">:</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">1000</span>]</span>
<span id="cb28-108"><a href="#cb28-108" aria-hidden="true" tabindex="-1"></a>        subset_y <span class="op">=</span> train_y[<span class="op">:</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">1000</span>]</span>
<span id="cb28-109"><a href="#cb28-109" aria-hidden="true" tabindex="-1"></a>        current_loss <span class="op">=</span> <span class="fu">loss_function</span>(model, subset_x, subset_y)</span>
<span id="cb28-110"><a href="#cb28-110" aria-hidden="true" tabindex="-1"></a>        <span class="fu">println</span>(<span class="st">"Epoch </span><span class="sc">$</span>(current_epoch)<span class="st"> complete. Sample training loss = </span><span class="sc">$</span>(current_loss)<span class="st">"</span>)</span>
<span id="cb28-111"><a href="#cb28-111" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-112"><a href="#cb28-112" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Save checkpoint at the given interval or on final epoch</span></span>
<span id="cb28-113"><a href="#cb28-113" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> (current_epoch <span class="op">%</span> checkpoint_interval <span class="op">==</span> <span class="fl">0</span>) <span class="op">||</span> (epoch <span class="op">==</span> epochs)</span>
<span id="cb28-114"><a href="#cb28-114" aria-hidden="true" tabindex="-1"></a>            model_params <span class="op">=</span> <span class="fu">cpu</span>.(Flux.<span class="fu">params</span>(model))</span>
<span id="cb28-115"><a href="#cb28-115" aria-hidden="true" tabindex="-1"></a>            filename <span class="op">=</span> <span class="st">"</span><span class="sc">$</span>(checkpoint_prefix)<span class="sc">$</span>(current_epoch)<span class="st">.bson"</span></span>
<span id="cb28-116"><a href="#cb28-116" aria-hidden="true" tabindex="-1"></a>            BSON.<span class="pp">@save</span> filename model_params current_epoch</span>
<span id="cb28-117"><a href="#cb28-117" aria-hidden="true" tabindex="-1"></a>            <span class="fu">println</span>(<span class="st">"Checkpoint saved for epoch </span><span class="sc">$</span>(current_epoch)<span class="st"> in file </span><span class="sc">$</span>(filename)<span class="st">"</span>)</span>
<span id="cb28-118"><a href="#cb28-118" aria-hidden="true" tabindex="-1"></a>        <span class="cf">end</span></span>
<span id="cb28-119"><a href="#cb28-119" aria-hidden="true" tabindex="-1"></a>    <span class="cf">end</span></span>
<span id="cb28-120"><a href="#cb28-120" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-121"><a href="#cb28-121" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> model, current_epoch</span>
<span id="cb28-122"><a href="#cb28-122" aria-hidden="true" tabindex="-1"></a><span class="kw">end</span></span>
<span id="cb28-123"><a href="#cb28-123" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-124"><a href="#cb28-124" aria-hidden="true" tabindex="-1"></a><span class="co"># ---------------------------------------------------------------------</span></span>
<span id="cb28-125"><a href="#cb28-125" aria-hidden="true" tabindex="-1"></a><span class="co"># Main training workflow</span></span>
<span id="cb28-126"><a href="#cb28-126" aria-hidden="true" tabindex="-1"></a>train_x, train_y, test_x, test_y <span class="op">=</span> <span class="fu">load_data</span>();</span>
<span id="cb28-127"><a href="#cb28-127" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> <span class="fu">build_model</span>()</span>
<span id="cb28-128"><a href="#cb28-128" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-129"><a href="#cb28-129" aria-hidden="true" tabindex="-1"></a><span class="co"># First phase: Train for 5 epochs and save checkpoint</span></span>
<span id="cb28-130"><a href="#cb28-130" aria-hidden="true" tabindex="-1"></a>first_phase_epochs <span class="op">=</span> <span class="fl">5</span></span>
<span id="cb28-131"><a href="#cb28-131" aria-hidden="true" tabindex="-1"></a>model, current_epoch <span class="op">=</span> <span class="fu">train_model!</span>(model, train_x, train_y; epochs<span class="op">=</span>first_phase_epochs, checkpoint_interval<span class="op">=</span>first_phase_epochs)</span>
<span id="cb28-132"><a href="#cb28-132" aria-hidden="true" tabindex="-1"></a>checkpoint_file <span class="op">=</span> <span class="st">"checkpoint_epoch</span><span class="sc">$</span>(current_epoch)<span class="st">.bson"</span></span>
<span id="cb28-133"><a href="#cb28-133" aria-hidden="true" tabindex="-1"></a><span class="fu">println</span>(<span class="st">"Checkpoint saved after training for </span><span class="sc">$</span>(current_epoch)<span class="st"> epoch(s)"</span>)</span>
<span id="cb28-134"><a href="#cb28-134" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-135"><a href="#cb28-135" aria-hidden="true" tabindex="-1"></a><span class="co"># Second phase: Reload checkpoint and retrain</span></span>
<span id="cb28-136"><a href="#cb28-136" aria-hidden="true" tabindex="-1"></a><span class="fu">println</span>(<span class="st">"Reloading checkpoint from file </span><span class="sc">$</span>(checkpoint_file)<span class="st">..."</span>)</span>
<span id="cb28-137"><a href="#cb28-137" aria-hidden="true" tabindex="-1"></a>BSON.<span class="pp">@load</span> checkpoint_file model_params current_epoch</span>
<span id="cb28-138"><a href="#cb28-138" aria-hidden="true" tabindex="-1"></a>model_params <span class="op">=</span> <span class="fu">cu</span>.(model_params)</span>
<span id="cb28-139"><a href="#cb28-139" aria-hidden="true" tabindex="-1"></a>Flux.<span class="fu">loadparams!</span>(model, model_params)</span>
<span id="cb28-140"><a href="#cb28-140" aria-hidden="true" tabindex="-1"></a><span class="fu">println</span>(<span class="st">"Checkpoint reloaded. Resuming training from epoch </span><span class="sc">$</span>(current_epoch <span class="op">+</span> <span class="fl">1</span>)<span class="st">"</span>)</span>
<span id="cb28-141"><a href="#cb28-141" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb28-142"><a href="#cb28-142" aria-hidden="true" tabindex="-1"></a><span class="co"># Continue training until a total of 10 epochs is reached</span></span>
<span id="cb28-143"><a href="#cb28-143" aria-hidden="true" tabindex="-1"></a>total_epochs <span class="op">=</span> <span class="fl">10</span></span>
<span id="cb28-144"><a href="#cb28-144" aria-hidden="true" tabindex="-1"></a>remaining_epochs <span class="op">=</span> total_epochs <span class="op">-</span> current_epoch</span>
<span id="cb28-145"><a href="#cb28-145" aria-hidden="true" tabindex="-1"></a>model, final_epoch <span class="op">=</span> <span class="fu">train_model!</span>(model, train_x, train_y; epochs<span class="op">=</span>remaining_epochs, checkpoint_interval<span class="op">=</span>remaining_epochs)</span>
<span id="cb28-146"><a href="#cb28-146" aria-hidden="true" tabindex="-1"></a><span class="fu">println</span>(<span class="st">"Training complete at epoch </span><span class="sc">$</span>(current_epoch <span class="op">+</span> final_epoch)<span class="st">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>