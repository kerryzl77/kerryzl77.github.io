---
title: ' Time Series Forecasting: From Iterative DeepAR to Direct Encoder-Decoder Transformers'
date: 2025-01-10
permalink: /posts/2014/08/blog-post-3/
tags:
  - deep learning
  - time series
  - transformers
---

Time series forecasting is a powerful tool for predicting future values based on past observations. In many real-world cases, it is crucial for tasks like supply chain management, demand forecasting, energy consumption prediction, and more. This blog post introduces two classes of approaches for multi-step ahead time series forecasting: 

1. **Iterative (Auto-Regressive) Forecasting**  
2. **Direct (All at Once) Forecasting**  

We will walk through the fundamentals of these approaches, highlight how **DeepAR** (with variations such as teacher forcing and ancestral sampling) and **Encoder-Decoder Transformers** fit into this paradigm, and present real experimental outcomes (MAE, MSE) from a Kaggle-like dataset.

--- 

## 1. Problem Statement

### 1.1 Iterative (Auto-Regressive) Forecasting
In *iterative* or *auto-regressive* forecasting, we predict one future time step at a time, then feed that prediction (or the ground truth during training) back into the model to predict the next step. Formally:

```math
\hat{y}_{t+1} = f(y_{t}, y_{t-1}, \dots, y_{t-L})
```

Then, to forecast $`\hat{y}_{t+2}`$, we use:

```math
\hat{y}_{t+2} = f(\hat{y}_{t+1}, y_{t}, \dots, y_{t-L+1})
```

This approach is simple and flexible but can accumulate errors because each new prediction depends on the previous predicted value(s). **DeepAR** is a prime example of a model that follows this iterative mechanism, particularly when it is run in prediction mode step-by-step.

---

### 1.2 Direct (All-at-Once) Forecasting
In *direct* multi-step forecasting, the model outputs all future steps in a single forward pass. You feed the historical context into the model, and it returns forecasts for multiple time steps $`\{t+1, t+2, \dots, t+H\}`$ at once. Formally:

```math
[\hat{y}_{t+1}, \ldots, \hat{y}_{t+H}] = g(y_{t}, y_{t-1}, \dots, y_{t-L})
```

This removes the iterative loop over future steps and mitigates error accumulation. The **Encoder-Decoder Transformer** approach we discuss below falls under this category, leveraging attention mechanisms to attend over the entire input sequence and predict multiple steps simultaneously.

---

## 2. DeepAR: Iterative Forecasting with RNNs

### 2.1 DeepAR Overview
[DeepAR](https://arxiv.org/abs/1704.04110) is an auto-regressive recurrent neural network model for probabilistic forecasting. Here is the rough outline:

- **RNN-based Encoder** that takes in past observations (plus any covariates).
- **Hidden State Propagation** to capture time dependencies.
- **Mean / Mean+Std Output** to represent the forecast distribution.

#### 2.1.1 Teacher Forcing
**Teacher forcing** is a training strategy where the model is fed the *ground truth* $`y_{t}`$ at each step instead of its *own prediction* $`\hat{y}_{t}`$. This often makes the training process more stable and helps the model converge faster—at the possible expense of *exposure bias*, since the model never learns to correct its own mistakes in the training loop.

<details>
<summary>Example Snippet (Pseudo-code)</summary>

```python
for t in range(seq_len + pred_len):
    if t == 0:
        prev_y = x_enc[:, 0, :]
    else:
        # Teacher Forcing: feed ground truth in training phase
        if self.is_train and t < seq_len:
            prev_y = x_enc[:, t-1, :]
        else:
            prev_y = pred_mean[:, t-1, :]  # use model's prediction
    ...
    # LSTM step
    out, hidden = self.lstm(input, hidden)
    mean_t = self.mean_layer(out)
    pred_mean[:, t, :] = mean_t
```
</details>

#### 2.1.2 Ancestral Sampling

When DeepAR predicts a distribution (e.g., Gaussian with mean μ and standard deviation σ), it can sample multiple paths from the distribution at each forecast step. This is called ancestral sampling, and it helps quantify uncertainty in the forecast:



