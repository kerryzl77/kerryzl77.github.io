---
title: ' Time Series Forecasting: From Iterative DeepAR to Direct Encoder-Decoder Transformers'
date: 2025-01-10
permalink: /posts/2014/08/blog-post-3/
tags:
  - deep learning
  - time series
  - transformers
---

Time series forecasting is a powerful tool for predicting future values based on past observations. In many real-world cases, it is crucial for tasks like supply chain management, demand forecasting, energy consumption prediction, and more. This blog post introduces two classes of approaches for multi-step ahead time series forecasting: 

1. **Iterative (Auto-Regressive) Forecasting**  
2. **Direct (All at Once) Forecasting**  

We will walk through the fundamentals of these approaches, highlight how **DeepAR** (with variations such as teacher forcing and ancestral sampling) and **Encoder-Decoder Transformers** fit into this paradigm, and present real experimental outcomes (MAE, MSE) from a Kaggle-like dataset.

--- 

## 1. Problem Statement

### 1.1 Iterative (Auto-Regressive) Forecasting
In *iterative* or *auto-regressive* forecasting, we predict one future time step at a time, then feed that prediction (or the ground truth during training) back into the model to predict the next step. Formally:

```math
\hat{y}_{t+1} = f(y_{t}, y_{t-1}, \dots, y_{t-L})
```

Then, to forecast $`\hat{y}_{t+2}`$, we use:

```math
\hat{y}_{t+2} = f(\hat{y}_{t+1}, y_{t}, \dots, y_{t-L+1})
```

This approach is simple and flexible but can accumulate errors because each new prediction depends on the previous predicted value(s). **DeepAR** is a prime example of a model that follows this iterative mechanism, particularly when it is run in prediction mode step-by-step.

---

### 1.2 Direct (All-at-Once) Forecasting
In *direct* multi-step forecasting, the model outputs all future steps in a single forward pass. You feed the historical context into the model, and it returns forecasts for multiple time steps $`\{t+1, t+2, \dots, t+H\}`$ at once. Formally:

```math
[\hat{y}_{t+1}, \ldots, \hat{y}_{t+H}] = g(y_{t}, y_{t-1}, \dots, y_{t-L})
```

This removes the iterative loop over future steps and mitigates error accumulation. The **Encoder-Decoder Transformer** approach we discuss below falls under this category, leveraging attention mechanisms to attend over the entire input sequence and predict multiple steps simultaneously.

---

## 2. DeepAR: Iterative Forecasting with RNNs

### 2.1 DeepAR Overview
[DeepAR](https://arxiv.org/abs/1704.04110) is an auto-regressive recurrent neural network model for probabilistic forecasting. Here is the rough outline:

- **RNN-based Encoder** that takes in past observations (plus any covariates).
- **Hidden State Propagation** to capture time dependencies.
- **Mean / Mean+Std Output** to represent the forecast distribution.

#### 2.1.1 Teacher Forcing
**Teacher forcing** is a training strategy where the model is fed the *ground truth* $`y_{t}`$ at each step instead of its *own prediction* $`\hat{y}_{t}`$. This often makes the training process more stable and helps the model converge faster—at the possible expense of *exposure bias*, since the model never learns to correct its own mistakes in the training loop.

<details>
<summary>Example Snippet (Pseudo-code)</summary>

```python
for t in range(seq_len + pred_len):
    if t == 0:
        prev_y = x_enc[:, 0, :]
    else:
        # Teacher Forcing: feed ground truth in training phase
        if self.is_train and t < seq_len:
            prev_y = x_enc[:, t-1, :]
        else:
            prev_y = pred_mean[:, t-1, :]  # use model's prediction
    ...
    # LSTM step
    out, hidden = self.lstm(input, hidden)
    mean_t = self.mean_layer(out)
    pred_mean[:, t, :] = mean_t
```
</details>

#### 2.1.2 Ancestral Sampling

When **DeepAR** predicts a distribution (e.g., Gaussian with mean \(\mu\) and standard deviation \(\sigma\)), it can sample multiple paths from the distribution at each forecast step. This is called **ancestral sampling**, and it helps quantify uncertainty in the forecast:


<br/><img src='/images/iterativeforecast.png'>

Over many samples, you get a distribution of forecasts that can be used to estimate confidence intervals. However, if you only evaluate point forecasts (e.g., MAE, MSE), a purely Gaussian NLL training objective might underperform direct MSE training because the model is forced to handle variability rather than strictly focusing on the best point estimate.


---

## 3. Encoder-Decoder Transformers: Direct Multi-Step Forecasting

### 3.1 Transformer Basics

Transformers use **self-attention** to learn dependencies between elements in a sequence. This architecture has two major components:

1. **Encoder**: Takes the input sequence and produces high-level representations.
2. **Decoder**: Consumes the encoder’s output (via cross-attention) and generates forecasts.

A standard *encoder-decoder transformer* can be formulated as:

\[
\text{EncOut} = \text{Encoder}(\mathbf{x}_{1:L})
\]
\[
\hat{\mathbf{y}}_{1:H} = \text{Decoder}(\text{EncOut}, \mathbf{mask})
\]

Here, \(\mathbf{x}_{1:L}\) are the past observations (plus any covariates), and \(\mathbf{mask}\) is used to prevent the model from peeking at future time steps in self-attention (e.g., in language modeling). For direct forecasting, the model outputs \(\hat{\mathbf{y}}_{1:H}\) in a single forward pass.

---

### 3.2 Positional Encodings & Time Covariates

Because Transformers do not have an inherent ordering (like RNN hidden states), positional encodings (sin-cos embeddings) or learnable embeddings for time indices are used. Additionally, we may include **time-dependent covariates** (e.g., day-of-week, holiday indicators, etc.) to enrich the input data.

<details>
<summary>Positional Encoding Pseudocode</summary>

```python
position = torch.arange(max_sequence_length).unsqueeze(1)
div_term = torch.exp(
    torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)
)
pe[:, 0::2] = torch.sin(position * div_term)
pe[:, 1::2] = torch.cos(position * div_term)
```
</details>

### 3.3 Informer, Autoformer, etc.

Several **extensions** of the vanilla Transformer architecture have emerged to address the high \(\mathcal{O}(N^2)\) complexity and better capture long-term dependencies:

- **Informer**: Introduces *ProbSparse* self-attention, which reduces complexity to \(\mathcal{O}(N \log N)\) by focusing on a subset of "informative" queries. This makes it feasible to handle much longer sequences.
- **Autoformer**: Employs a series decomposition block to separate seasonal and trend components. The seasonal part is processed with attention mechanisms, while trends are modeled with average pooling layers to capture smooth variations.
- **Reformer**: Utilizes locality-sensitive hashing (LSH) to compress attention computation and reversible residual layers to reduce memory usage.

For our experiments, we tested a more standard **Encoder-Decoder Transformer** with full attention. The implementation can be found in the [Enc_Dec_Transformer.py](https://github.com/kerryzl77/Time-Series-Capstone/blob/main/Enc_Dec_Transformer.py) file.

---

## 4. Dataset & Experimental Setup

### 4.1 Dataset

We used a dataset inspired by the [M5 Forecasting competition](https://www.kaggle.com/competitions/m5-forecasting-accuracy). The dataset represents sales or demand over time, with the following characteristics:

- **Sequence Length**: Look-back window of **90 time steps** for training.
- **Prediction Horizon**: Forecast **10 time steps** ahead.
- **Features**: Included time-related features such as:
  - Day of the week
  - Day of the month
  - Holiday indicators
  - Optional static metadata (e.g., category or location information).

This setup is typical for retail or e-commerce forecasting tasks where both time-related patterns and external covariates play a role.

---

### 4.2 Model Training

All models were implemented using **PyTorch** and trained with the following configuration:

- **Batch Size**: 32
- **Learning Rate**: \(1 \times 10^{-4}\)
- **Dropout/Weight Decay**: 0.1–0.3 (to prevent overfitting)

#### Key Scripts
- **`exp_forecasting_TF.py`**: Used for training **DeepAR** with teacher forcing.
- **`exp_forecasting.py`**: Used for training **DeepAR_AS** (ancestral sampling) and **Encoder-Decoder Transformers**.

To run an experiment (e.g., training DeepAR with teacher forcing), use the following command:

```bash
python exp_forecasting_TF.py \
  --seq_len 90 \
  --pred_len 10 \
  --batch_size 32 \
  --learning_rate 1e-4 \
  --model "DeepAR_TF"
```
## 5. Experimental Results

### 5.1 Test Set Metrics (MAE, MSE)

Below is a summary of the results for different models evaluated over a 10-step forecast horizon:

| **Model**                              | **MAE** | **MSE**   |
|----------------------------------------|--------:|----------:|
| **Linear Baseline**                    | 138.37  | 51,745    |
| **DeepAR (Mean)**                      | 141.23  | 57,402    |
| **DeepAR (Mean + Teacher Forcing)**    | 131.09  | 50,762    |
| **DeepAR (Single-path Ancestral)**     | 221.00  | 129,036   |
| **DeepAR (100-path Ancestral)**        | 209.00  | 118,332   |
| **Encoder-Decoder Transformer**        | 119.12  | 37,700    |
| **Encoder-Decoder Transformer + Time** | 122.10  | 40,849    |

---

### 5.2 Key Observations

- **Encoder-Decoder Transformer** achieved the best performance, with the lowest **MAE** and **MSE**, due to its **direct multi-step forecasting approach**.
- **DeepAR (Mean + Teacher Forcing)** was competitive, particularly in point-forecast metrics (MAE=131.09).
- **DeepAR with Ancestral Sampling** underperformed in point metrics but excels in capturing forecast uncertainty, as demonstrated by its probabilistic outputs.
- The **Linear Baseline** performed surprisingly well, highlighting the strong predictive power of simple methods on certain datasets. This aligns with findings from the paper *Are Transformers Effective for Time Series Forecasting?*.

---

## 6. Next Steps and Future Directions

### 6.1 Probabilistic Calibration

While point forecasts (MAE, MSE) are useful, real-world applications often require **uncertainty quantification**. Models like **DeepAR_AS** can be evaluated using additional metrics:

- **Continuous Ranked Probability Score (CRPS)**: Measures the quality of probabilistic forecasts.
- **Prediction Interval Coverage**: Checks whether the ground truth falls within the predicted confidence intervals.

---

### 6.2 Advanced Feature Engineering

Incorporating richer temporal features or embeddings could further improve performance. Examples include:

- **Yearly trends**
- **Business calendar effects**
- **Hierarchical embeddings** for product categories, locations, or other metadata.

Methods like **Autoformer** leverage local patterns using Fourier transforms and seasonal decomposition, offering promising extensions.

---

### 6.3 Scalability and Parallelization

- **DeepAR** is inherently sequential due to its RNN-based architecture, making it slower for long horizons.
- **Transformer-based models** can forecast the entire horizon in parallel but face high memory usage. Approaches like **Informer** (\(\mathcal{O}(N \log N)\)) offer more scalable alternatives for massive sequences.

---

### 6.4 Longer Sequence Lengths

Increasing sequence lengths can improve performance for Transformers but may also introduce challenges like:

- **Memory constraints**: Longer sequences require more GPU memory for training.
- **Overfitting risks**: With insufficient regularization, the model may overfit to long input sequences.

Fine-tuning sequence lengths (e.g., **400–1000 steps**) for specific datasets is a valuable direction.

---

## 7. Code and Reproducibility

All code is publicly available on GitHub, and training logs/metrics were tracked using **Weights & Biases**.

- **GitHub Repository**: [Time-Series-Capstone](https://github.com/kerryzl77/Time-Series-Capstone)

To run an experiment, clone the repository and execute one of the provided scripts. For example, to train **DeepAR** with teacher forcing:

```bash
python exp_forecasting_TF.py \
  --seq_len 90 \
  --pred_len 10 \
  --batch_size 32 \
  --learning_rate 1e-4 \
  --model "DeepAR_TF"
```
